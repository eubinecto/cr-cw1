{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WWWn1oMdMP0d"
   },
   "source": [
    "MLP for Complex Problems: The MNIST dataset\n",
    "=========\n",
    "\n",
    "\n",
    "In this exercise we will first learn to use the simple perceptron network (input-output layers only) and a Multi-Layer Perceptron (MLP, with one or more hidden layers). To make the task more interesting than the XOR problem, we will be using a more complex training set. This will be the MNIST dataset, a well known neural network problem for the recognition of the 10 handwritten characters from 0 to 9 ([MNIST](http://yann.lecun.com/exdb/mnist/)).\n",
    "\n",
    "This exercise is based on the  Gulli & Pal (2017) 'Deep Learning with Keras' textbook, with some additional code to help us understand and test the programme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ezwPEU5UMP0o"
   },
   "source": [
    "**Importing the libraries and defining the main training parameters**\n",
    "\n",
    "The initial code is necessary to prepare the data and the simulation (hyper)parameters.\n",
    "We first import numpy. In our case we will use it to create and pre-process the array of the training data sets. We then import a few functions from Keras (we used some of these in our previous XOR exercise). The matplotlib library will be used for visualising some MNIST images and the plot of the training results.\n",
    "\n",
    "The code also defines the variables for some of the main parameters used throughout this program. \n",
    "The random seed definition is also important to be able to repeat the same parameter configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YpM98ckGMP0u"
   },
   "outputs": [],
   "source": [
    "# import of numpy and keras libraries\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# variables for network and training\n",
    "N_EPOCH = 200 # initially set at 200 ; you can change this later \n",
    "BATCH_SIZE = 128  \n",
    "VERBOSE = 1\n",
    "N_CLASSES = 10   # number of classes/categories of digits from 0 to 9, i.e. number of output units \n",
    "OPTIMIZER = SGD(lr=0.1) # Stochastic gradient descent optimiser\n",
    "N_HIDDEN = 128   # number of hidden units\n",
    "VALIDATION_SPLIT=0.2 # proportion of the dataset used for validation, with remaining .8 for training \n",
    "\n",
    "#each 2D image consists of 28x28 values/pixels, which needs to be reshaped in a vector of 784 pixels\n",
    "RESHAPED = 784\n",
    "\n",
    "# random seed number to be used for reproducibility\n",
    "np.random.seed(1671)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rgZEImMMP08"
   },
   "source": [
    "**Preparing the MNIST dataset and visualising the input images**\n",
    "\n",
    "This part of the code prepares the input and output training set, and the corresponding test sets. \n",
    "It also visualises a sample image. The MNIST dataset is included in the Keras program and we do not need to use and external file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4-TuW2_MP0-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data input shape:  (60000, 28, 28)\n",
      "Training data output shape:  (60000,)\n",
      "Test data input shape:  (10000, 28, 28)\n",
      "Test data ouput shape:  (10000,)\n",
      "Sample input image: [[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0  67 232  39   0   0   0   0   0]\n",
      " [  0   0   0   0  62  81   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0 120 180  39   0   0   0   0   0]\n",
      " [  0   0   0   0 126 163   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   2 153 210  40   0   0   0   0   0]\n",
      " [  0   0   0   0 220 163   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0  27 254 162   0   0   0   0   0   0]\n",
      " [  0   0   0   0 222 163   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0 183 254 125   0   0   0   0   0   0]\n",
      " [  0   0   0  46 245 163   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0 198 254  56   0   0   0   0   0   0]\n",
      " [  0   0   0 120 254 163   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   23 231 254  29   0   0   0   0   0   0]\n",
      " [  0   0   0 159 254 120   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  163 254 216  16   0   0   0   0   0   0]\n",
      " [  0   0   0 159 254  67   0   0   0   0   0   0   0   0   0  14  86 178\n",
      "  248 254  91   0   0   0   0   0   0   0]\n",
      " [  0   0   0 159 254  85   0   0   0  47  49 116 144 150 241 243 234 179\n",
      "  241 252  40   0   0   0   0   0   0   0]\n",
      " [  0   0   0 150 253 237 207 207 207 253 254 250 240 198 143  91  28   5\n",
      "  233 250   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0 119 177 177 177 177 177  98  56   0   0   0   0   0 102\n",
      "  254 220   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  254 137   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  254  57   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  254  57   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  255  94   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  254  96   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  254 153   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 169\n",
      "  255 153   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  96\n",
      "  254 153   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# data: shuffled and split between train and test sets, loading and using the Keras mnist dataset\n",
    "(input_X_train, output_Y_train), (input_X_test, output_Y_test) = mnist.load_data()\n",
    "\n",
    "# print the shapes of the input and output data\n",
    "print(\"Training data input shape: \" , input_X_train.shape)\n",
    "print(\"Training data output shape: \" , output_Y_train.shape)\n",
    "print(\"Test data input shape: \" , input_X_test.shape)\n",
    "print(\"Test data ouput shape: \" , output_Y_test.shape)\n",
    "\n",
    "# visualisation of the numerical vector and plot of a selected image\n",
    "Selected_Image = 2\n",
    "image = input_X_train[Selected_Image]\n",
    "print (\"Sample input image: \" + str(image))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cDqNl3taMP1P"
   },
   "source": [
    "The input images now have to be reshaped as a linear vector. That is, we go from a 2D image of 28x28 pixels, to a linear vector of 784 (i.e. 28*28) pixels, to be passed as the 784 input units. Moreover, the initial pixel grey values given as type __int__ in the range 0-255 will be normalised to the __float32__ type in the range 0-1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fB15FrXOMP1R"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data ready\n"
     ]
    }
   ],
   "source": [
    "# use 60000 images for training, 10000 for validation test\n",
    "input_X_train = input_X_train.reshape(60000, RESHAPED)\n",
    "input_X_test = input_X_test.reshape(10000, RESHAPED)\n",
    "input_X_train = input_X_train.astype('float32')\n",
    "input_X_test = input_X_test.astype('float32')\n",
    "\n",
    "# normalisation of the pixel values from 0-255 range to 0-1 range \n",
    "input_X_train /= 255\n",
    "input_X_test /= 255\n",
    "\n",
    "print (\"Input data ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsCwC5y7MP1Z"
   },
   "source": [
    "**Preparing the output labels**\n",
    "\n",
    "This code converts the output data into categorical (one-hot encoding) vectors of 0s and 1s.\n",
    "See example of the visualisation of the one-hot vector for the selected image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rc4dgAqqMP1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot-vector: [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "output_Y_train = np_utils.to_categorical(output_Y_train, N_CLASSES)\n",
    "output_Y_test = np_utils.to_categorical(output_Y_test, N_CLASSES)\n",
    "\n",
    "# print the categorical, one-hot output vector for the sample image\n",
    "label = output_Y_train[Selected_Image]\n",
    "print (\"One-hot-vector: \" + str(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h-rcBfv8MP1q"
   },
   "source": [
    "Training the Simple Perceptron\n",
    "=========\n",
    "\n",
    "**Defining the network: Simple Perceptron**\n",
    "\n",
    "We will start by training a simple perceptron, i.e. a network with an input layer (the 784 input values/pixels) connected to the output layer (the 10 number classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HTb4RC7zMP1s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defaults sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Dense layer for all to all connections\n",
    "# Define the output layer with 10 output units, and softmax activation as categorical output\n",
    "model.add(Dense(N_CLASSES, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Use categorical crossentropy for the loss evaluation, and the accuracy metrics\n",
    "# we previously chose the SGD optimiser in the OPTIMIZER variable definition  \n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n",
    "\n",
    "#show the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uHyq0YoRMP1y"
   },
   "source": [
    "**Let's train the simple perceptron network**\n",
    "\n",
    "Let's now train (fit) the network with the above-defined batch size (128), and number of epochs (200). We save the training results into the history variable.\n",
    "\n",
    "Here we use the previous __VALIDATION_SPLIT=0.2__ definition to split the dataset into a 20% (0.2) validation set and the remaning 80% as training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lg3RCFukMP10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.6275 - accuracy: 0.8419 - val_loss: 0.3982 - val_accuracy: 0.8966\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3963 - accuracy: 0.8916 - val_loss: 0.3491 - val_accuracy: 0.9056\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.3595 - accuracy: 0.9005 - val_loss: 0.3283 - val_accuracy: 0.9098\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3405 - accuracy: 0.9053 - val_loss: 0.3159 - val_accuracy: 0.9134\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.3284 - accuracy: 0.9085 - val_loss: 0.3080 - val_accuracy: 0.9151\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s 21us/step - loss: 0.3196 - accuracy: 0.9106 - val_loss: 0.3024 - val_accuracy: 0.9159\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.3129 - accuracy: 0.9129 - val_loss: 0.2983 - val_accuracy: 0.9162\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3075 - accuracy: 0.9146 - val_loss: 0.2934 - val_accuracy: 0.9183\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.3031 - accuracy: 0.9153 - val_loss: 0.2925 - val_accuracy: 0.9183\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2995 - accuracy: 0.9167 - val_loss: 0.2887 - val_accuracy: 0.9187\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2962 - accuracy: 0.9177 - val_loss: 0.2859 - val_accuracy: 0.9209\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2936 - accuracy: 0.9176 - val_loss: 0.2852 - val_accuracy: 0.9202\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s 18us/step - loss: 0.2911 - accuracy: 0.9178 - val_loss: 0.2841 - val_accuracy: 0.9206\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2890 - accuracy: 0.9193 - val_loss: 0.2823 - val_accuracy: 0.9209\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2868 - accuracy: 0.9199 - val_loss: 0.2817 - val_accuracy: 0.9212\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.2850 - accuracy: 0.9199 - val_loss: 0.2792 - val_accuracy: 0.9233\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s 23us/step - loss: 0.2835 - accuracy: 0.9205 - val_loss: 0.2798 - val_accuracy: 0.9222\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s 19us/step - loss: 0.2818 - accuracy: 0.9214 - val_loss: 0.2779 - val_accuracy: 0.9220\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s 22us/step - loss: 0.2804 - accuracy: 0.9215 - val_loss: 0.2776 - val_accuracy: 0.9223\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2793 - accuracy: 0.9220 - val_loss: 0.2784 - val_accuracy: 0.9216\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.2780 - accuracy: 0.9219 - val_loss: 0.2759 - val_accuracy: 0.9227\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2767 - accuracy: 0.9230 - val_loss: 0.2746 - val_accuracy: 0.9234\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.2758 - accuracy: 0.9226 - val_loss: 0.2739 - val_accuracy: 0.9234\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2749 - accuracy: 0.9237 - val_loss: 0.2735 - val_accuracy: 0.9240\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2737 - accuracy: 0.9236 - val_loss: 0.2736 - val_accuracy: 0.9247\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.2729 - accuracy: 0.9239 - val_loss: 0.2729 - val_accuracy: 0.9235\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s 17us/step - loss: 0.2719 - accuracy: 0.9237 - val_loss: 0.2722 - val_accuracy: 0.9252\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.2712 - accuracy: 0.9242 - val_loss: 0.2731 - val_accuracy: 0.9247\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2705 - accuracy: 0.9246 - val_loss: 0.2720 - val_accuracy: 0.9242\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2697 - accuracy: 0.9245 - val_loss: 0.2716 - val_accuracy: 0.9242\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2691 - accuracy: 0.9246 - val_loss: 0.2707 - val_accuracy: 0.9252\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2686 - accuracy: 0.9251 - val_loss: 0.2707 - val_accuracy: 0.9250\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2675 - accuracy: 0.9254 - val_loss: 0.2706 - val_accuracy: 0.9252\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2671 - accuracy: 0.9256 - val_loss: 0.2702 - val_accuracy: 0.9252\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2664 - accuracy: 0.9254 - val_loss: 0.2708 - val_accuracy: 0.9252\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2661 - accuracy: 0.9253 - val_loss: 0.2688 - val_accuracy: 0.9262\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2653 - accuracy: 0.9260 - val_loss: 0.2695 - val_accuracy: 0.9242\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2647 - accuracy: 0.9262 - val_loss: 0.2698 - val_accuracy: 0.9263\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2643 - accuracy: 0.9268 - val_loss: 0.2689 - val_accuracy: 0.9258\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2636 - accuracy: 0.9266 - val_loss: 0.2696 - val_accuracy: 0.9253\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2633 - accuracy: 0.9265 - val_loss: 0.2691 - val_accuracy: 0.9263\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2627 - accuracy: 0.9266 - val_loss: 0.2682 - val_accuracy: 0.9264\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2623 - accuracy: 0.9268 - val_loss: 0.2689 - val_accuracy: 0.9265\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2620 - accuracy: 0.9265 - val_loss: 0.2684 - val_accuracy: 0.9262\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2615 - accuracy: 0.9268 - val_loss: 0.2685 - val_accuracy: 0.9267\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2610 - accuracy: 0.9270 - val_loss: 0.2678 - val_accuracy: 0.9262\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2607 - accuracy: 0.9276 - val_loss: 0.2688 - val_accuracy: 0.9258\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2602 - accuracy: 0.9278 - val_loss: 0.2685 - val_accuracy: 0.9263\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2600 - accuracy: 0.9274 - val_loss: 0.2674 - val_accuracy: 0.9270\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2594 - accuracy: 0.9276 - val_loss: 0.2671 - val_accuracy: 0.9266\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2591 - accuracy: 0.9275 - val_loss: 0.2681 - val_accuracy: 0.9273\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2586 - accuracy: 0.9275 - val_loss: 0.2668 - val_accuracy: 0.9277\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2584 - accuracy: 0.9281 - val_loss: 0.2669 - val_accuracy: 0.9268\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2582 - accuracy: 0.9281 - val_loss: 0.2669 - val_accuracy: 0.9275\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2577 - accuracy: 0.9283 - val_loss: 0.2674 - val_accuracy: 0.9270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2573 - accuracy: 0.9282 - val_loss: 0.2665 - val_accuracy: 0.9278\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2571 - accuracy: 0.9284 - val_loss: 0.2674 - val_accuracy: 0.9275\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2568 - accuracy: 0.9280 - val_loss: 0.2670 - val_accuracy: 0.9263\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2565 - accuracy: 0.9288 - val_loss: 0.2667 - val_accuracy: 0.9274\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2561 - accuracy: 0.9289 - val_loss: 0.2675 - val_accuracy: 0.9273\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2561 - accuracy: 0.9291 - val_loss: 0.2663 - val_accuracy: 0.9270\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2556 - accuracy: 0.9291 - val_loss: 0.2676 - val_accuracy: 0.9262\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2552 - accuracy: 0.9287 - val_loss: 0.2669 - val_accuracy: 0.9271\n",
      "Epoch 64/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2551 - accuracy: 0.9287 - val_loss: 0.2660 - val_accuracy: 0.9269\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2547 - accuracy: 0.9290 - val_loss: 0.2661 - val_accuracy: 0.9281\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2544 - accuracy: 0.9289 - val_loss: 0.2662 - val_accuracy: 0.9275\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2542 - accuracy: 0.9294 - val_loss: 0.2660 - val_accuracy: 0.9283\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2539 - accuracy: 0.9296 - val_loss: 0.2657 - val_accuracy: 0.9285\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2535 - accuracy: 0.9292 - val_loss: 0.2668 - val_accuracy: 0.9273\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2533 - accuracy: 0.9292 - val_loss: 0.2667 - val_accuracy: 0.9270\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2532 - accuracy: 0.9294 - val_loss: 0.2659 - val_accuracy: 0.9278\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2532 - accuracy: 0.9296 - val_loss: 0.2657 - val_accuracy: 0.9278\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2525 - accuracy: 0.9297 - val_loss: 0.2671 - val_accuracy: 0.9269\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2524 - accuracy: 0.9292 - val_loss: 0.2666 - val_accuracy: 0.9281\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2521 - accuracy: 0.9300 - val_loss: 0.2667 - val_accuracy: 0.9281\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2519 - accuracy: 0.9295 - val_loss: 0.2669 - val_accuracy: 0.9275\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2517 - accuracy: 0.9294 - val_loss: 0.2660 - val_accuracy: 0.9285\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2516 - accuracy: 0.9300 - val_loss: 0.2655 - val_accuracy: 0.9293\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2512 - accuracy: 0.9297 - val_loss: 0.2656 - val_accuracy: 0.9282\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2511 - accuracy: 0.9300 - val_loss: 0.2661 - val_accuracy: 0.9281\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2510 - accuracy: 0.9298 - val_loss: 0.2668 - val_accuracy: 0.9285\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2506 - accuracy: 0.9305 - val_loss: 0.2665 - val_accuracy: 0.9281\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2505 - accuracy: 0.9309 - val_loss: 0.2663 - val_accuracy: 0.9270\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2503 - accuracy: 0.9300 - val_loss: 0.2660 - val_accuracy: 0.9285\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2503 - accuracy: 0.9304 - val_loss: 0.2660 - val_accuracy: 0.9290\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2500 - accuracy: 0.9305 - val_loss: 0.2668 - val_accuracy: 0.9273\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2497 - accuracy: 0.9302 - val_loss: 0.2651 - val_accuracy: 0.9285\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2497 - accuracy: 0.9305 - val_loss: 0.2660 - val_accuracy: 0.9278\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2491 - accuracy: 0.9302 - val_loss: 0.2663 - val_accuracy: 0.9277\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2491 - accuracy: 0.9309 - val_loss: 0.2660 - val_accuracy: 0.9285\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2491 - accuracy: 0.9304 - val_loss: 0.2669 - val_accuracy: 0.9287\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2488 - accuracy: 0.9308 - val_loss: 0.2656 - val_accuracy: 0.9287\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2487 - accuracy: 0.9303 - val_loss: 0.2658 - val_accuracy: 0.9280\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2484 - accuracy: 0.9310 - val_loss: 0.2662 - val_accuracy: 0.9285\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2482 - accuracy: 0.9308 - val_loss: 0.2663 - val_accuracy: 0.9282\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2481 - accuracy: 0.9308 - val_loss: 0.2659 - val_accuracy: 0.9287\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2480 - accuracy: 0.9307 - val_loss: 0.2655 - val_accuracy: 0.9284\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2477 - accuracy: 0.9306 - val_loss: 0.2656 - val_accuracy: 0.9288\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2476 - accuracy: 0.9308 - val_loss: 0.2654 - val_accuracy: 0.9282\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2474 - accuracy: 0.9310 - val_loss: 0.2664 - val_accuracy: 0.9283\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2473 - accuracy: 0.9306 - val_loss: 0.2660 - val_accuracy: 0.9284\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2471 - accuracy: 0.9310 - val_loss: 0.2666 - val_accuracy: 0.9272\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2469 - accuracy: 0.9315 - val_loss: 0.2662 - val_accuracy: 0.9290\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2468 - accuracy: 0.9311 - val_loss: 0.2665 - val_accuracy: 0.9293\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2466 - accuracy: 0.9310 - val_loss: 0.2665 - val_accuracy: 0.9273\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2467 - accuracy: 0.9321 - val_loss: 0.2659 - val_accuracy: 0.9282\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2463 - accuracy: 0.9310 - val_loss: 0.2667 - val_accuracy: 0.9274\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2461 - accuracy: 0.9311 - val_loss: 0.2659 - val_accuracy: 0.9287\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2462 - accuracy: 0.9311 - val_loss: 0.2662 - val_accuracy: 0.9283\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2459 - accuracy: 0.9315 - val_loss: 0.2667 - val_accuracy: 0.9280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2459 - accuracy: 0.9316 - val_loss: 0.2660 - val_accuracy: 0.9285\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2456 - accuracy: 0.9316 - val_loss: 0.2663 - val_accuracy: 0.9279\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2456 - accuracy: 0.9319 - val_loss: 0.2660 - val_accuracy: 0.9287\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2454 - accuracy: 0.9316 - val_loss: 0.2659 - val_accuracy: 0.9283\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2452 - accuracy: 0.9314 - val_loss: 0.2679 - val_accuracy: 0.9271\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2452 - accuracy: 0.9320 - val_loss: 0.2656 - val_accuracy: 0.9293\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2449 - accuracy: 0.9315 - val_loss: 0.2663 - val_accuracy: 0.9283\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2446 - accuracy: 0.9319 - val_loss: 0.2668 - val_accuracy: 0.9287\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2448 - accuracy: 0.9314 - val_loss: 0.2660 - val_accuracy: 0.9281\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2446 - accuracy: 0.9321 - val_loss: 0.2665 - val_accuracy: 0.9277\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2444 - accuracy: 0.9319 - val_loss: 0.2676 - val_accuracy: 0.9278\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2443 - accuracy: 0.9320 - val_loss: 0.2659 - val_accuracy: 0.9283\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2442 - accuracy: 0.9321 - val_loss: 0.2665 - val_accuracy: 0.9279\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2441 - accuracy: 0.9318 - val_loss: 0.2683 - val_accuracy: 0.9277\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2442 - accuracy: 0.9320 - val_loss: 0.2666 - val_accuracy: 0.9282\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2439 - accuracy: 0.9320 - val_loss: 0.2662 - val_accuracy: 0.9280\n",
      "Epoch 127/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2437 - accuracy: 0.9321 - val_loss: 0.2673 - val_accuracy: 0.9279\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2437 - accuracy: 0.9320 - val_loss: 0.2663 - val_accuracy: 0.9289\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2435 - accuracy: 0.9322 - val_loss: 0.2659 - val_accuracy: 0.9288\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2435 - accuracy: 0.9318 - val_loss: 0.2652 - val_accuracy: 0.9290\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2431 - accuracy: 0.9326 - val_loss: 0.2675 - val_accuracy: 0.9268\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s 16us/step - loss: 0.2433 - accuracy: 0.9317 - val_loss: 0.2668 - val_accuracy: 0.9287\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s 15us/step - loss: 0.2430 - accuracy: 0.9325 - val_loss: 0.2669 - val_accuracy: 0.9285\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2430 - accuracy: 0.9322 - val_loss: 0.2676 - val_accuracy: 0.9265\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2428 - accuracy: 0.9323 - val_loss: 0.2664 - val_accuracy: 0.9277\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2426 - accuracy: 0.9322 - val_loss: 0.2671 - val_accuracy: 0.9280\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2427 - accuracy: 0.9326 - val_loss: 0.2670 - val_accuracy: 0.9283\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2424 - accuracy: 0.9324 - val_loss: 0.2662 - val_accuracy: 0.9289\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2424 - accuracy: 0.9324 - val_loss: 0.2672 - val_accuracy: 0.9277\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2423 - accuracy: 0.9327 - val_loss: 0.2664 - val_accuracy: 0.9283\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2423 - accuracy: 0.9321 - val_loss: 0.2664 - val_accuracy: 0.9284\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2420 - accuracy: 0.9325 - val_loss: 0.2675 - val_accuracy: 0.9280\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2421 - accuracy: 0.9323 - val_loss: 0.2662 - val_accuracy: 0.9287\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2418 - accuracy: 0.9330 - val_loss: 0.2672 - val_accuracy: 0.9273\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2419 - accuracy: 0.9330 - val_loss: 0.2663 - val_accuracy: 0.9283\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2416 - accuracy: 0.9326 - val_loss: 0.2673 - val_accuracy: 0.9281\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2415 - accuracy: 0.9326 - val_loss: 0.2666 - val_accuracy: 0.9280\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2414 - accuracy: 0.9331 - val_loss: 0.2673 - val_accuracy: 0.9283\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2414 - accuracy: 0.9325 - val_loss: 0.2670 - val_accuracy: 0.9285\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2414 - accuracy: 0.9335 - val_loss: 0.2659 - val_accuracy: 0.9293\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2413 - accuracy: 0.9325 - val_loss: 0.2670 - val_accuracy: 0.9288\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2411 - accuracy: 0.9329 - val_loss: 0.2667 - val_accuracy: 0.9281\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2409 - accuracy: 0.9331 - val_loss: 0.2662 - val_accuracy: 0.9294\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2410 - accuracy: 0.9327 - val_loss: 0.2666 - val_accuracy: 0.9288\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2407 - accuracy: 0.9330 - val_loss: 0.2664 - val_accuracy: 0.9287\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2407 - accuracy: 0.9336 - val_loss: 0.2668 - val_accuracy: 0.9286\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2407 - accuracy: 0.9336 - val_loss: 0.2671 - val_accuracy: 0.9272\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2405 - accuracy: 0.9328 - val_loss: 0.2663 - val_accuracy: 0.9279\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2403 - accuracy: 0.9333 - val_loss: 0.2674 - val_accuracy: 0.9275\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2402 - accuracy: 0.9335 - val_loss: 0.2673 - val_accuracy: 0.9278\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2402 - accuracy: 0.9332 - val_loss: 0.2673 - val_accuracy: 0.9278\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2402 - accuracy: 0.9328 - val_loss: 0.2673 - val_accuracy: 0.9292\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2400 - accuracy: 0.9333 - val_loss: 0.2683 - val_accuracy: 0.9276\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2400 - accuracy: 0.9331 - val_loss: 0.2676 - val_accuracy: 0.9281\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2398 - accuracy: 0.9335 - val_loss: 0.2671 - val_accuracy: 0.9283\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2397 - accuracy: 0.9332 - val_loss: 0.2680 - val_accuracy: 0.9272\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2398 - accuracy: 0.9337 - val_loss: 0.2664 - val_accuracy: 0.9287\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2395 - accuracy: 0.9338 - val_loss: 0.2674 - val_accuracy: 0.9283\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2396 - accuracy: 0.9334 - val_loss: 0.2681 - val_accuracy: 0.9285\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2394 - accuracy: 0.9332 - val_loss: 0.2674 - val_accuracy: 0.9290\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2394 - accuracy: 0.9339 - val_loss: 0.2681 - val_accuracy: 0.9278\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2392 - accuracy: 0.9333 - val_loss: 0.2664 - val_accuracy: 0.9293\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2392 - accuracy: 0.9335 - val_loss: 0.2676 - val_accuracy: 0.9284\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2391 - accuracy: 0.9334 - val_loss: 0.2678 - val_accuracy: 0.9278\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2390 - accuracy: 0.9335 - val_loss: 0.2677 - val_accuracy: 0.9283\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2390 - accuracy: 0.9341 - val_loss: 0.2681 - val_accuracy: 0.9284\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2389 - accuracy: 0.9336 - val_loss: 0.2681 - val_accuracy: 0.9272\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2388 - accuracy: 0.9338 - val_loss: 0.2684 - val_accuracy: 0.9277\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2388 - accuracy: 0.9337 - val_loss: 0.2688 - val_accuracy: 0.9283\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2388 - accuracy: 0.9336 - val_loss: 0.2675 - val_accuracy: 0.9288\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2384 - accuracy: 0.9342 - val_loss: 0.2670 - val_accuracy: 0.9292\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2384 - accuracy: 0.9339 - val_loss: 0.2671 - val_accuracy: 0.9285\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2383 - accuracy: 0.9336 - val_loss: 0.2675 - val_accuracy: 0.9281\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2384 - accuracy: 0.9339 - val_loss: 0.2673 - val_accuracy: 0.9285\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2383 - accuracy: 0.9339 - val_loss: 0.2677 - val_accuracy: 0.9283\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2382 - accuracy: 0.9344 - val_loss: 0.2669 - val_accuracy: 0.9291\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2380 - accuracy: 0.9340 - val_loss: 0.2679 - val_accuracy: 0.9282\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2380 - accuracy: 0.9335 - val_loss: 0.2678 - val_accuracy: 0.9280\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2379 - accuracy: 0.9341 - val_loss: 0.2677 - val_accuracy: 0.9283\n",
      "Epoch 190/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2379 - accuracy: 0.9334 - val_loss: 0.2684 - val_accuracy: 0.9279\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2378 - accuracy: 0.9339 - val_loss: 0.2673 - val_accuracy: 0.9293\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2375 - accuracy: 0.9339 - val_loss: 0.2689 - val_accuracy: 0.9270\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s 14us/step - loss: 0.2377 - accuracy: 0.9339 - val_loss: 0.2684 - val_accuracy: 0.9278\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2376 - accuracy: 0.9341 - val_loss: 0.2676 - val_accuracy: 0.9277\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2375 - accuracy: 0.9345 - val_loss: 0.2682 - val_accuracy: 0.9291\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s 13us/step - loss: 0.2374 - accuracy: 0.9341 - val_loss: 0.2681 - val_accuracy: 0.9281\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2372 - accuracy: 0.9342 - val_loss: 0.2685 - val_accuracy: 0.9278\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2374 - accuracy: 0.9340 - val_loss: 0.2682 - val_accuracy: 0.9285\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2374 - accuracy: 0.9346 - val_loss: 0.2682 - val_accuracy: 0.9284\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s 12us/step - loss: 0.2370 - accuracy: 0.9340 - val_loss: 0.2685 - val_accuracy: 0.9286\n"
     ]
    }
   ],
   "source": [
    "#train the network\n",
    "history = model.fit(input_X_train, output_Y_train, batch_size=BATCH_SIZE, epochs=N_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2MpCg_YTMP17"
   },
   "source": [
    "**Looking at the results of the trained network**\n",
    "\n",
    "Let's evaluate the model to see how well it has learned ( or not) the MNIST problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZN9rro7KMP19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 35us/step\n",
      "\n",
      "Test score/loss: 0.27114191164970397\n",
      "Test accuracy: 0.9254000186920166\n"
     ]
    }
   ],
   "source": [
    "#test the network using the generalisation test dataset\n",
    "score = model.evaluate(input_X_test, output_Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score/loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GZuRJvfwMP2K"
   },
   "source": [
    "Training the Multi-Layer Perceptron\n",
    "=========\n",
    "**Defining the network: Multi-Layer Perceptron**\n",
    "\n",
    "We will now create a multi-layer perceptron with 784 input units, two hidden layers with 128 hidden units each, and an output layer with the 10 units.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HoF0ib-SMP2N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "N_EPOCH = 20 # we need fewer epoch than before, as the multi-layer percetpron can learn faster.\n",
    "N_HIDDEN = 128\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Hidden layer 1 with 128 hidden units and ReLu activation function\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "# Hidden layer 2 with 128 hidden units and ReLu activation function\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# output layer with 10 units and softmax activation\n",
    "model.add(Dense(N_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Summary of the whole model\n",
    "model.summary()\n",
    "\n",
    "# model compilation\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OgYzi3hyMP2V"
   },
   "source": [
    "**Let's train the mulri-layer perceptron network**\n",
    "\n",
    "Let's now train (fit) the network with the above-defined batch size (128), and number of epochs (20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iNa3cmoaMP2X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.4912 - accuracy: 0.8636 - val_loss: 0.2570 - val_accuracy: 0.9256\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.2401 - accuracy: 0.9295 - val_loss: 0.1966 - val_accuracy: 0.9434\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1841 - accuracy: 0.9466 - val_loss: 0.1645 - val_accuracy: 0.9521\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 32us/step - loss: 0.1504 - accuracy: 0.9559 - val_loss: 0.1382 - val_accuracy: 0.9620\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.1264 - accuracy: 0.9638 - val_loss: 0.1280 - val_accuracy: 0.9644\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 24us/step - loss: 0.1088 - accuracy: 0.9689 - val_loss: 0.1205 - val_accuracy: 0.9664\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.0943 - accuracy: 0.9726 - val_loss: 0.1085 - val_accuracy: 0.9684\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0843 - accuracy: 0.9759 - val_loss: 0.1150 - val_accuracy: 0.9670\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0742 - accuracy: 0.9786 - val_loss: 0.1054 - val_accuracy: 0.9689\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.0679 - accuracy: 0.9806 - val_loss: 0.0982 - val_accuracy: 0.9710\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.0606 - accuracy: 0.9827 - val_loss: 0.0986 - val_accuracy: 0.9724\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.0548 - accuracy: 0.9846 - val_loss: 0.0897 - val_accuracy: 0.9734\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 35us/step - loss: 0.0492 - accuracy: 0.9860 - val_loss: 0.0908 - val_accuracy: 0.9718\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 38us/step - loss: 0.0444 - accuracy: 0.9871 - val_loss: 0.0863 - val_accuracy: 0.9755\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 1s 31us/step - loss: 0.0407 - accuracy: 0.9887 - val_loss: 0.0859 - val_accuracy: 0.974814 - accura\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 1s 26us/step - loss: 0.0371 - accuracy: 0.9900 - val_loss: 0.0854 - val_accuracy: 0.9753\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0332 - accuracy: 0.9911 - val_loss: 0.0918 - val_accuracy: 0.9722\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0295 - accuracy: 0.9926 - val_loss: 0.0891 - val_accuracy: 0.9748\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0273 - accuracy: 0.9934 - val_loss: 0.0885 - val_accuracy: 0.9753\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 1s 25us/step - loss: 0.0246 - accuracy: 0.9940 - val_loss: 0.0863 - val_accuracy: 0.9761\n"
     ]
    }
   ],
   "source": [
    "#train the network\n",
    "history = model.fit(input_X_train, output_Y_train, batch_size=BATCH_SIZE, epochs=N_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kV5v4LNjMP2e"
   },
   "source": [
    "**Looking at the results of the trained network**\n",
    "\n",
    "Let's explotre the results both for the score and accuracy values, as well as to visualise the plots of these values during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CXpu8p9uMP2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 29us/step\n",
      "\n",
      "Test score/loss: 0.0750860998799093\n",
      "Test accuracy: 0.9768000245094299\n"
     ]
    }
   ],
   "source": [
    "#test the network\n",
    "score = model.evaluate(input_X_test, output_Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score/loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYRZkVf6MP2t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJ9YbKviMP2z"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CV-aShF1MP24"
   },
   "source": [
    "**Adding weight droputs**\n",
    "\n",
    "An efficient way to reduce the number of parameters (weights) to be trained, as well as to increase generalisation capabilities, is to randomly remove (i.e. __dropout__) a certain proportion of the nodes at random in each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4GNrMiVoMP25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# import the dropout layer type\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "# Probability of weights dropout\n",
    "P_DROPOUT = 0.3\n",
    "\n",
    "# We can increse this parameter afterwards\n",
    "N_EPOCH = 20\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(P_DROPOUT))\n",
    "model.add(Dense(N_HIDDEN))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(P_DROPOUT))\n",
    "model.add(Dense(N_CLASSES))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# model compilation\n",
    "model.summary()\n",
    "\n",
    "# model compilation\n",
    "model.compile(loss='categorical_crossentropy', optimizer=OPTIMIZER, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NBJJWwueMP2-"
   },
   "source": [
    "**Let's train the multi-layer perceptron network with DROPOUT**\n",
    "\n",
    "Let's now train (fit) the above dropout network with the above-defined batch size (128), and number of epochs (250)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xqa-9UHxMP3A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.6713 - accuracy: 0.7961 - val_loss: 0.2639 - val_accuracy: 0.9241\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.3363 - accuracy: 0.9002 - val_loss: 0.1957 - val_accuracy: 0.9413\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 1s 29us/step - loss: 0.2712 - accuracy: 0.9198 - val_loss: 0.1684 - val_accuracy: 0.9511\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.2315 - accuracy: 0.9316 - val_loss: 0.1466 - val_accuracy: 0.9565\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.2045 - accuracy: 0.9401 - val_loss: 0.1299 - val_accuracy: 0.9622\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.1837 - accuracy: 0.9456 - val_loss: 0.1236 - val_accuracy: 0.9642\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 1s 27us/step - loss: 0.1678 - accuracy: 0.9496 - val_loss: 0.1109 - val_accuracy: 0.9678\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 1s 28us/step - loss: 0.1550 - accuracy: 0.9538 - val_loss: 0.1061 - val_accuracy: 0.9693\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 2s 36us/step - loss: 0.1478 - accuracy: 0.9557 - val_loss: 0.1048 - val_accuracy: 0.9703\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 2s 39us/step - loss: 0.1396 - accuracy: 0.9582 - val_loss: 0.0970 - val_accuracy: 0.9724\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.1308 - accuracy: 0.9606 - val_loss: 0.0948 - val_accuracy: 0.9718\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.1268 - accuracy: 0.9609 - val_loss: 0.0918 - val_accuracy: 0.9733\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 2s 40us/step - loss: 0.1178 - accuracy: 0.9642 - val_loss: 0.0966 - val_accuracy: 0.9731\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 2s 45us/step - loss: 0.1142 - accuracy: 0.9655 - val_loss: 0.0863 - val_accuracy: 0.9749\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 2s 41us/step - loss: 0.1124 - accuracy: 0.9660 - val_loss: 0.0858 - val_accuracy: 0.9739\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 2s 33us/step - loss: 0.1062 - accuracy: 0.9677 - val_loss: 0.0835 - val_accuracy: 0.9756\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 2s 34us/step - loss: 0.1017 - accuracy: 0.9699 - val_loss: 0.0835 - val_accuracy: 0.9767\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 2s 37us/step - loss: 0.0990 - accuracy: 0.9693 - val_loss: 0.0821 - val_accuracy: 0.9763\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 2s 42us/step - loss: 0.0955 - accuracy: 0.9707 - val_loss: 0.0809 - val_accuracy: 0.9762\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.0932 - accuracy: 0.9702 - val_loss: 0.0825 - val_accuracy: 0.9759\n"
     ]
    }
   ],
   "source": [
    "#train the network\n",
    "history = model.fit(input_X_train, output_Y_train, batch_size=BATCH_SIZE, epochs=N_EPOCH, verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x-z1NP88MP3I"
   },
   "source": [
    "**Looking at the results of the trained dropout network**\n",
    "\n",
    "Let's explore the effects of adding the weight dropout on the network performance.\n",
    "\n",
    "You can see that the dropout has further improved our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K8qIacDmMP3N"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 31us/step\n",
      "\n",
      "Test score: 0.08064564523221925\n",
      "Test accuracy: 0.9754999876022339\n",
      "dict_keys(['val_loss', 'val_accuracy', 'loss', 'accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#test the network\n",
    "score = model.evaluate(input_X_test, output_Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8IUIcenlMP3S"
   },
   "source": [
    "Exploring Training Hyperparameters\n",
    "-------------\n",
    "\n",
    "You can explore the role of various hyperparameters to see how you can further improve the MLP model's performance on the MNIST dataset. \n",
    "\n",
    "For example, if you increase the number of epochs for the dropout network to 250, you will see that the test and train accuracy errors will converge (accuracy closer to 97% for both training and test), which means that we have achieved the best tradeoff between training and testing.\n",
    "\n",
    "You can carry out many additional simulations on hypeparameter exploration where you can try for example:\n",
    "\n",
    "- different number of epochs\n",
    "- different learning rate\n",
    "- different number of hidden nodes \n",
    "- different proportion of dropout rates \n",
    "- different optimisers in addition to SGD (e.g. RMSprop, Adam) \n",
    "- different batch size \n",
    "\n",
    "This final exercise will constitute the first neural network coursework. See Coursework specification documenty for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uPKj5NAdMP3T"
   },
   "source": [
    "Conclusions\n",
    "-------------\n",
    "\n",
    "With this tutorial we have practiced the training of both a Simple Perceptron, and a Multi-Layer Perceptron, with a benchmark dataset containing images of handwritten numbers.\n",
    "This helped us understand how to load the datase, visualise it, and visualise the training history and the effects of adding hidden layers and then adding weight dropout.\n",
    "\n",
    "**Copyright (c)** 2019 Angelo Cangelosi, MIT License. Code and examples adapted from Gulli & Pal (2017) Deep Learning with Keras. Punkt Publishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L879MYIcMP3V"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab5b_Keras_MLP_MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
