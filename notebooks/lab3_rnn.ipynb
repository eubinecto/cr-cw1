{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15"
    },
    "colab": {
      "name": "Lab3_RNN.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpnRb1rNp6mv"
      },
      "source": [
        "Recurrent Neural Networks (RNN): Simple RNNs and LSTMs\n",
        "=========\n",
        "\n",
        "In this tutorial we will learn how to use Recurremt Neural Networks (RNNs) for text processing.\n",
        "The first example uses a simple RNN to generate the characters in a small text corpus (Alice in Wonderland novel).\n",
        "We will then look at the Long Short Term Memory (LSTM) networks, for more advanced recurrent models. The LSTM will be tested on a sentiment analysis corpus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjaUzq1ip6m1"
      },
      "source": [
        "1 - Simple RNNs to Generate text\n",
        "=========\n",
        "\n",
        "This exmple will use a simple RNN with one layer of hidden and recurrent units. \n",
        "\n",
        "Let's first import the usual keras and utility functions. This will include the first importing and use of the SimpleRNN recurrent layer type in Keras.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx1fsJ45p6nA"
      },
      "source": [
        "from __future__ import print_function\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.layers.recurrent import SimpleRNN\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIdP2MSmp6nP"
      },
      "source": [
        "**Loading text file and converting to clean text**\n",
        "\n",
        "This code will read the file \"alice_in_wonderland.txt\". This file is available for download here http://www.gutenberg.org/files/11/11-0.txt (and can also be downloaded from Balckboard ). \n",
        "\n",
        "The code will do some preliminary cleanup of the text (e.g. removing non-ASCII characters and line breaks) and write all words in the variable called \"whole_text\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtQ15qnCp6nS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cacc3826-8761-439c-e3ae-0fb37f55656d"
      },
      "source": [
        "print(\"Loading text file...\")\n",
        "\n",
        "INPUT_FILE = \"alice_in_wonderland.txt\"\n",
        "\n",
        "fin = open(INPUT_FILE, 'rb')\n",
        "lines = []\n",
        "for line in fin:\n",
        "    line = line.strip().lower()\n",
        "    line = line.decode(\"ascii\", \"ignore\")\n",
        "    if len(line) == 0:\n",
        "        continue\n",
        "    lines.append(line)\n",
        "fin.close()\n",
        "whole_text = \" \".join(lines)\n",
        "\n",
        "print(\"Done!\")\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading text file...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzRl-KiBp6nh"
      },
      "source": [
        "**Characters look-up table**\n",
        "\n",
        "This code will create the look-up table from the 42 characters to integer IDs, and viceversa. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzNWK_g1p6nj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f05d71f-ca32-470f-e5fa-8eed7e45c34b"
      },
      "source": [
        "print(\"Preparing characters look-up table...\")\n",
        "\n",
        "chars = set([c for c in whole_text])\n",
        "nb_chars = len(chars)\n",
        "char2index = dict((c, i) for i, c in enumerate(chars))\n",
        "index2char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preparing characters look-up table...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5VKYsImp6nv"
      },
      "source": [
        "**Input text and output character**\n",
        "\n",
        "To create the input text, the code will step through the whole text by a number of characters defined by the __STEP__ variable (1 in our case) and then extract a set of characters whose size is determined by the __SEQLEN__ variable (10 in our case). The next character after the extracted characters is the output label, i,e. the next character to predict.\n",
        "\n",
        "Here is an example of the input/output for the part of text starting as\" it turned into a pig\" \n",
        "\n",
        "   INPUT (10)    ->   OUTPUT (1)\n",
        "- \"it turned \"   ->   i\n",
        "- \"t turned i\"   ->   n\n",
        "- \" turned in\"   ->   t\n",
        "- \"turned int\"   ->   o\n",
        "- \"urned into\"   ->\n",
        "- \"rned into \"   ->   a\n",
        "- \"ned into a\"   ->\n",
        "- \"ed into a \"   ->   p\n",
        "- \"d into a p\"   ->   i\n",
        "- \" into a pi\"   ->   g\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyS8tFKsp6nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbdba0e7-4b09-47e2-d3e6-ea7c28e4a089"
      },
      "source": [
        "print(\"Creating input and label text...\")\n",
        "\n",
        "SEQLEN = 10\n",
        "STEP = 1\n",
        "\n",
        "input_chars = []\n",
        "label_chars = []\n",
        "for i in range(0, len(whole_text) - SEQLEN, STEP):\n",
        "    input_chars.append(whole_text[i:i + SEQLEN])\n",
        "    label_chars.append(whole_text[i + SEQLEN])\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating input and label text...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaTGaKHap6n_"
      },
      "source": [
        "**Vectorisation of the input and output**\n",
        "\n",
        "This preprares the input and output vectors of the training set. \n",
        "The input vector uses one-hot encoding of the __SEQLEN__ (10) characters present in the input text segment, times the __nb_chars__ (42) possible characters.\n",
        "The output label uses one-hot encoding for the activation of a single character out of the __nb_chars__ number of units/characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssALtUABp6oC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0fb5f1c-bf02-4e2d-8b97-7572475dba1f"
      },
      "source": [
        "print(\"Vectorizing input and label text...\")\n",
        "\n",
        "X = np.zeros((len(input_chars), SEQLEN, nb_chars), dtype=np.bool)\n",
        "y = np.zeros((len(input_chars), nb_chars), dtype=np.bool)\n",
        "for i, input_char in enumerate(input_chars):\n",
        "    for j, ch in enumerate(input_char):\n",
        "        X[i, j, char2index[ch]] = 1\n",
        "    y[i, char2index[label_chars[i]]] = 1\n",
        "\n",
        "print(\"Done!\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorizing input and label text...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZKv1Xwep6oJ"
      },
      "source": [
        "**Simple RNN CNN definition**\n",
        "\n",
        "This codes defines the key variables of the model and training, and creates the sequential model. The model consists of a simple recurrent neural network (RNN) with a hidden layer of 128 simple recurrent units.\n",
        "\n",
        "The __return_sequences__ is set to __False__ as the output only consists of one character, and not a sequence. \n",
        "The __unroll=True__ setting improves performance on the TensorFlow backend.\n",
        "The optimiser uses the __rmsprop__ for backpropagation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZFdn5ynp6oL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33004af4-8e54-45d5-bd5e-a37ae220003d"
      },
      "source": [
        "# Definition of the network and training hyperparameters\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "NUM_ITERATIONS = 25\n",
        "NUM_EPOCHS_PER_ITERATION = 1\n",
        "NUM_PREDS_PER_EPOCH = 100\n",
        "\n",
        "# Definition of the network topology, with simpleRNN hidden layer\n",
        "model = Sequential()\n",
        "model.add(SimpleRNN(HIDDEN_SIZE, return_sequences=False, input_shape=(SEQLEN, nb_chars), unroll=True))\n",
        "model.add(Dense(nb_chars))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\")\n",
        "\n",
        "#show the model summary\n",
        "model.summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_3 (SimpleRNN)     (None, 128)               23424     \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 54)                6966      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 54)                0         \n",
            "=================================================================\n",
            "Total params: 30,390\n",
            "Trainable params: 30,390\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrEyYv-hp6oR"
      },
      "source": [
        "**Training and testing of the model**\n",
        "\n",
        "The model is trained for __NUM_ITERATIONS__ (25) epochs and tested after each epoch, to allow us to monitor the improvement of the model performance in character prediction.\n",
        "\n",
        "The test consists of generating a character from the model given a random input, then dropping the first character from the input and appending the predicted character from the previous run as the new input, to generate another character. This is done for __NUM_PREDS_PER_EPOCH__ (100) steps. The completed string gives us an indication of the quality of the model's processing of English words (within the limited lexicon of Alice's novel).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKeEI_hup6oT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbbfb428-45ff-4cca-9c71-72748bfeb110"
      },
      "source": [
        "for iteration in range(NUM_ITERATIONS):\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Iteration #: %d\" % (iteration))\n",
        "    model.fit(X, y, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS_PER_ITERATION)\n",
        "    \n",
        "    # testing model\n",
        "    # randomly choose a row from input_chars, then use it to \n",
        "    # generate text from model for next 100 chars\n",
        "    test_idx = np.random.randint(len(input_chars))\n",
        "    test_chars = input_chars[test_idx]\n",
        "    print(\"Generating from seed: %s\" % (test_chars))\n",
        "    print(test_chars, end=\"\")\n",
        "    for i in range(NUM_PREDS_PER_EPOCH):\n",
        "        Xtest = np.zeros((1, SEQLEN, nb_chars))\n",
        "        for i, ch in enumerate(test_chars):\n",
        "            Xtest[0, i, char2index[ch]] = 1\n",
        "        pred = model.predict(Xtest, verbose=0)[0]\n",
        "        ypred = index2char[np.argmax(pred)]\n",
        "        print(ypred, end=\"\")\n",
        "        # move forward with test_chars + ypred\n",
        "        test_chars = test_chars[1:] + ypred\n",
        "    print()\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "Iteration #: 0\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 49us/step - loss: 2.3335\n",
            "Generating from seed:  branch of\n",
            " branch of and the said the said the said the said the said the said the said the said the said the said the s\n",
            "==================================================\n",
            "Iteration #: 1\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 2.0456\n",
            "Generating from seed: hese stran\n",
            "hese strand the gromed to the wast the was the wast the was the wast the was the wast the was the wast the was\n",
            "==================================================\n",
            "Iteration #: 2\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.9394\n",
            "Generating from seed: lings. i q\n",
            "lings. i queen the lithen the wast an it the mast and the say said alice was she was so the was so the was so \n",
            "==================================================\n",
            "Iteration #: 3\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 7s 47us/step - loss: 1.8585\n",
            "Generating from seed: xcept thos\n",
            "xcept thos down the mare the mare the mare the mare the mare the mare the mare the mare the mare the mare the \n",
            "==================================================\n",
            "Iteration #: 4\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 50us/step - loss: 1.7934\n",
            "Generating from seed: hought at \n",
            "hought at the dont and she had ere the dinter all she had it was a little the dont and she had ere the dinter \n",
            "==================================================\n",
            "Iteration #: 5\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 49us/step - loss: 1.7396\n",
            "Generating from seed: the direct\n",
            "the directed to the dont was and her and the cater all the cane alice was see was the was the was the was the \n",
            "==================================================\n",
            "Iteration #: 6\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.6942\n",
            "Generating from seed:  when i wa\n",
            " when i was the latter it was so the looked and the looked and the looked and the looked and the looked and th\n",
            "==================================================\n",
            "Iteration #: 7\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.6553\n",
            "Generating from seed: cheshire c\n",
            "cheshire could the coure she was and the coure she was and the coure she was and the coure she was and the cou\n",
            "==================================================\n",
            "Iteration #: 8\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.6223\n",
            "Generating from seed: tone of gr\n",
            "tone of grawn do work the looked and the queen the dormouse of the gryphon and she said the queen the dormouse\n",
            "==================================================\n",
            "Iteration #: 9\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 49us/step - loss: 1.5931\n",
            "Generating from seed: k at the f\n",
            "k at the foond the hatter what it was the did the did the did the did the did the did the did the did the did \n",
            "==================================================\n",
            "Iteration #: 10\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.5685\n",
            "Generating from seed: ase, which\n",
            "ase, which the mound of the hatter the more that it was a the gryphon the mound of the hatter the more that it\n",
            "==================================================\n",
            "Iteration #: 11\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 49us/step - loss: 1.5462\n",
            "Generating from seed:  proud as \n",
            " proud as it was a little the partoned the caterpillar the caterpillar the caterpillar the caterpillar the cat\n",
            "==================================================\n",
            "Iteration #: 12\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.5266\n",
            "Generating from seed: left foot,\n",
            "left foot, and the dormouse said the dormouse said the dormouse said the dormouse said the dormouse said the d\n",
            "==================================================\n",
            "Iteration #: 13\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.5095\n",
            "Generating from seed: he fifth b\n",
            "he fifth began to the caterpillar the mouse was not at the caterpillar the mouse was not at the caterpillar th\n",
            "==================================================\n",
            "Iteration #: 14\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.4936\n",
            "Generating from seed: rudeness w\n",
            "rudeness work in a little said alice, and the caterpillar she cat replied and the project gutenberg-tm electro\n",
            "==================================================\n",
            "Iteration #: 15\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.4789\n",
            "Generating from seed: after the \n",
            "after the more the gryphon the mouse was a little the court she had been the more the gryphon the mouse was a \n",
            "==================================================\n",
            "Iteration #: 16\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.4671\n",
            "Generating from seed: d be quite\n",
            "d be quite a compliance turnled at the caterpillar. the gryphon the dormouse said the queen the dormouse said \n",
            "==================================================\n",
            "Iteration #: 17\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.4550\n",
            "Generating from seed: d alice, (\n",
            "d alice, (alice was and she went on the simple she had been the dormouse was a little began was a little began\n",
            "==================================================\n",
            "Iteration #: 18\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.4450\n",
            "Generating from seed: no idea wh\n",
            "no idea what they would not the tround to be she had been the mouse soon a grown began to the formation of the\n",
            "==================================================\n",
            "Iteration #: 19\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.4359\n",
            "Generating from seed: nd raised \n",
            "nd raised the queen the queen the queen the queen the queen the queen the queen the queen the queen the queen \n",
            "==================================================\n",
            "Iteration #: 20\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.4268\n",
            "Generating from seed: he mouse. \n",
            "he mouse. the mock turtle she was so the head alice to the caterpillar the dormouse was the mock turtle she wa\n",
            "==================================================\n",
            "Iteration #: 21\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.4190\n",
            "Generating from seed:  voice of \n",
            " voice of the same a she had been the mock turtle so the mouse go the hatter what it was and the mouse go the \n",
            "==================================================\n",
            "Iteration #: 22\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 49us/step - loss: 1.4114\n",
            "Generating from seed:  the cater\n",
            " the caterpillar. i dont like the caterpillar. i dont like the caterpillar. i dont like the caterpillar. i don\n",
            "==================================================\n",
            "Iteration #: 23\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 49us/step - loss: 1.4033\n",
            "Generating from seed: the jury, \n",
            "the jury, and the project gutenberg-tm works in the project gutenberg-tm works in the project gutenberg-tm wor\n",
            "==================================================\n",
            "Iteration #: 24\n",
            "Epoch 1/1\n",
            "158172/158172 [==============================] - 8s 48us/step - loss: 1.3972\n",
            "Generating from seed: ng the sea\n",
            "ng the sear it was of the began the same a great of the work in a shant the project gutenberg-tm electronic wo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXjvnFK6p6og"
      },
      "source": [
        "This example shows how we can train a Simple RNN to predict the next charaters, using the information from the sequentail relationship between words in the training text.\n",
        "You can try the same code on a different, longer text, or try different hyperparameters.\n",
        "\n",
        "We are now ready to try a more complex RNN architecture, for more complex time series tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY5Pe1aCp6oi"
      },
      "source": [
        "2 - LSTM for Sentiment Analysis\n",
        "=========\n",
        "\n",
        "This example will show how to code the more complex RNN model of the Long Short Term Memmory (LSTM) network. This tutorial will use the example of sentiment analysis prediction.\n",
        "\n",
        "Let's start with the usual importing of Keras and utility modules. This will of course include the LSTM recurrent layer type.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJqODAGwp6ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8b97e9-fd36-4030-d8a4-f3bdde5f29d6"
      },
      "source": [
        "from keras.layers.core import Activation, Dense, Dropout, SpatialDropout1D\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.recurrent import LSTM\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt') # download instruction required if pubkt nltk package not installed\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "print(\"Initialisation Done! Make sure you have initialised ntlk punkt\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Initialisation Done! Make sure you have initialised ntlk punkt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8a2MWghp6oq"
      },
      "source": [
        "Let's now define the main hyperparameter values and text corpus details.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__8WvCXSp6ot"
      },
      "source": [
        "INPUT_FILE = \"umich-sentiment-train.txt\"\n",
        "\n",
        "EMBEDDING_SIZE = 128\n",
        "HIDDEN_LAYER_SIZE = 64\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpXCUzwep6oz"
      },
      "source": [
        "**Analysis of words in corpus**\n",
        "\n",
        "This code load the text from the UMICH labelled sentiment analysis corpus file, to identify the number of unique words (length of the __word_freq__ variable, i.e. 2313) and the max number of words in the longest sentence (__maxlen__ expected to be 42). These two variables will be needed later for the preaparation of the training set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E63yJkoMp6o1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c93f1d6c-36d0-44f3-d876-0e14048c74d3"
      },
      "source": [
        "# Read training data and generate vocabulary\n",
        "maxlen = 0\n",
        "word_freqs = collections.Counter()\n",
        "num_recs = 0\n",
        "\n",
        "ftrain = open(INPUT_FILE, 'rb')\n",
        "for line in ftrain:\n",
        "    label, sentence = line.strip().split(b\"\\t\")\n",
        "    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n",
        "    if len(words) > maxlen:\n",
        "        maxlen = len(words)\n",
        "    for word in words:\n",
        "        word_freqs[word] += 1\n",
        "    num_recs += 1\n",
        "ftrain.close()\n",
        "\n",
        "## Get some information about our corpus\n",
        "print (maxlen)            # 42\n",
        "print (len(word_freqs))   # 2313"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "42\n",
            "2313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiqZt7Dlp6pB"
      },
      "source": [
        "**Unkown word and padding word and lookup tables**\n",
        "\n",
        "Knowing the number of unique words will permit the identification of the out of vocabulary (OOV) words, which can be replaced by the pseudo-word \"UNK\" (for unknown). When testing the network prediction, this will allow us to handle previously unseen OOV words.\n",
        "\n",
        "We can decide a fixed sequence length by truncating longer sentences to that length as appropriate (__MAX_SENTENCE_LENGTH = 40__). We also use the PAD to make shorter sentence equivalent to the fixed max length.\n",
        "\n",
        "Overall, we set our VOCABULARY_SIZE to 2002. i.e. 2,000 words from our vocabulary plus the UNK pseudo-word and the PAD pseudo word used for padding sentences up to the max length.\n",
        "\n",
        "We then create the look-up tables. Each row of input to the network is a sequence of word indices, where the indices are ordered by most frequent to least frequent word in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abPruGn0p6pD"
      },
      "source": [
        "MAX_FEATURES = 2000\n",
        "MAX_SENTENCE_LENGTH = 40\n",
        "\n",
        "# 1 is UNK, 0 is PAD\n",
        "# We take MAX_FEATURES-1 featurs to accound for PAD\n",
        "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
        "word2index = {x[0]: i+2 for i, x in \n",
        "                enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
        "word2index[\"PAD\"] = 0\n",
        "word2index[\"UNK\"] = 1\n",
        "index2word = {v:k for k, v in word2index.items()}\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivjEnz11p6pI"
      },
      "source": [
        "**Prepare training sequences**\n",
        "\n",
        "The input sentences need to be converted to  word index sequences (inlcuding their padiing to the max sequecne length). \n",
        "No procesing required for the output, as this is a positive/negative binary sentiment output.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA9RgxaDp6pK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "386d1df5-2641-4e08-f0b6-7cf05858ec51"
      },
      "source": [
        "# convert sentences to sequences\n",
        "X = np.empty((num_recs, ), dtype=list)\n",
        "y = np.zeros((num_recs, ))\n",
        "i = 0\n",
        "ftrain = open(INPUT_FILE, 'rb')\n",
        "for line in ftrain:\n",
        "    label, sentence = line.strip().split(b\"\\t\")\n",
        "    words = nltk.word_tokenize(sentence.decode(\"ascii\", \"ignore\").lower())\n",
        "    seqs = []\n",
        "    for word in words:\n",
        "        if word in word2index:\n",
        "            seqs.append(word2index[word])\n",
        "        else:\n",
        "            seqs.append(word2index[\"UNK\"])\n",
        "    X[i] = seqs\n",
        "    y[i] = int(label)\n",
        "    i += 1\n",
        "ftrain.close()\n",
        "\n",
        "# Pad the sequences (left padded with zeros)\n",
        "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)\n",
        "\n",
        "# Split input into training and test\n",
        "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5668, 40) (1418, 40) (5668,) (1418,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x4dQwv_p6pQ"
      },
      "source": [
        "**CNN building and compilation**\n",
        "\n",
        "The codes defines the LSTM model topology, as per figure below. This is a stdnard recurrent network with one layer of LSTM units.\n",
        "\n",
        "![image.png](attachment:image.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MKE0uglp6pR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e21e9e7-f43d-42fa-d14a-02575b2e7ef3"
      },
      "source": [
        "# Build model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, EMBEDDING_SIZE, input_length=MAX_SENTENCE_LENGTH))\n",
        "model.add(SpatialDropout1D(0.2))\n",
        "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1))\n",
        "model.add(Activation(\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "#show the model summary\n",
        "model.summary()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 40, 128)           256256    \n",
            "_________________________________________________________________\n",
            "spatial_dropout1d_1 (Spatial (None, 40, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 65        \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 305,729\n",
            "Trainable params: 305,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRy2C7YJp6pV"
      },
      "source": [
        "**Training of the model**\n",
        "\n",
        "Let's train our LSTM RNN.\n",
        "The model uses the binary cross-entropy loss function, suitable for binary value prediction. The Adam optimizer is one of the best and most frequently used general purpose optimizers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDc6PUGfp6pX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58e260c-848f-4b47-8457-8aee55103d71"
      },
      "source": [
        "history = model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, validation_data=(Xtest, ytest))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5668 samples, validate on 1418 samples\n",
            "Epoch 1/10\n",
            "5668/5668 [==============================] - 7s 1ms/step - loss: 0.2499 - accuracy: 0.8911 - val_loss: 0.0708 - val_accuracy: 0.9753\n",
            "Epoch 2/10\n",
            "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0314 - accuracy: 0.9901 - val_loss: 0.0439 - val_accuracy: 0.9859\n",
            "Epoch 3/10\n",
            "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0092 - accuracy: 0.9974 - val_loss: 0.0442 - val_accuracy: 0.9887\n",
            "Epoch 4/10\n",
            "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0079 - accuracy: 0.9974 - val_loss: 0.0455 - val_accuracy: 0.9873\n",
            "Epoch 5/10\n",
            "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0461 - val_accuracy: 0.9838\n",
            "Epoch 6/10\n",
            "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0024 - accuracy: 0.9996 - val_loss: 0.0551 - val_accuracy: 0.9887\n",
            "Epoch 7/10\n",
            "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 0.0488 - val_accuracy: 0.9894\n",
            "Epoch 8/10\n",
            "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 0.0892 - val_accuracy: 0.9760\n",
            "Epoch 9/10\n",
            "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0624 - val_accuracy: 0.9852\n",
            "Epoch 10/10\n",
            "5668/5668 [==============================] - 6s 1ms/step - loss: 0.0021 - accuracy: 0.9995 - val_loss: 0.0557 - val_accuracy: 0.9873\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwTrIcuLp6pk"
      },
      "source": [
        "**Plotting of the results**\n",
        "\n",
        "The code below will produce accuracy and loss plots for training and validation data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1c-1aJFp6pl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "outputId": "9b9f7032-c060-4df7-9750-2720f1d4a7cd"
      },
      "source": [
        "# plot loss and accuracy\n",
        "plt.subplot(211)\n",
        "plt.title(\"Accuracy\")\n",
        "plt.plot(history.history[\"accuracy\"], color=\"g\", label=\"Train\")\n",
        "plt.plot(history.history[\"val_accuracy\"], color=\"b\", label=\"Validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.title(\"Loss\")\n",
        "plt.plot(history.history[\"loss\"], color=\"g\", label=\"Train\")\n",
        "plt.plot(history.history[\"val_loss\"], color=\"b\", label=\"Validation\")\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAIABJREFUeJzt3Xl8VPW9//HXJ5NAQhLIgqxBgoiyk01AWZRiLdpW1FILiopWsbTWqnVB9GqvFqWt5aK2tXVB6wZVLK33V5X2Vipgq5KwySKCEDQkQAg7IZDl8/vjTJJJyDJJJjmTmc/z8TiPmTnbfOZkMu/5nvOdc0RVMcYYY4JNhNsFGGOMMXWxgDLGGBOULKCMMcYEJQsoY4wxQckCyhhjTFCygDLGGBOULKCMMcYEJQsoYxohIv8SkYMi0tHtWowJJxZQxjRARFKBcYACl7fh80a21XMZE6wsoIxp2PXAR8BLwA2VI0UkRkR+LSK7ROSwiKwSkRjvtLEi8m8ROSQiX4nIDO/4f4nIzT7rmCEiq3weq4j8SES2Adu84570ruOIiOSIyDif+T0iMkdEvhCRo97pfUTktyLya98XISJvi8idrbGBjGktFlDGNOx64DXv8A0R6e4d/wSQCVwAJAH3AhUi0hd4F3gaOANIA9Y14fmuAEYBg72PV3vXkQS8DrwpItHeaXcB04DLgM7ATUAx8EdgmohEAIhIV+Bi7/LGtBsWUMbUQ0TGAn2BN1Q1B/gCuMb7wX8T8BNV3a2q5ar6b1U9CVwD/J+qLlLVUlUtUtWmBNTjqnpAVU8AqOqr3nWUqeqvgY7Aud55bwYeVNWt6ljvnfcT4DAw0TvfVOBfqrq3hZvEmDZlAWVM/W4A/q6q+72PX/eO6wpE4wRWbX3qGe+vr3wfiMjdIrLFuxvxENDF+/yNPdcfgene+9OBV1pQkzGusAOxxtTBezzpasAjInu8ozsCCUBPoAToD6yvtehXwMh6Vnsc6OTzuEcd81RdXsB7vOlenJbQJlWtEJGDgPg8V39gYx3reRXYKCIjgEHAX+qpyZigZS0oY+p2BVCOcywozTsMAlbiHJdaCMwXkV7ezgrne7uhvwZcLCJXi0ikiCSLSJp3neuAq0Skk4icDXy/kRrigTKgEIgUkYdwjjVVeh54VEQGiGO4iCQDqGoezvGrV4C3KncZGtOeWEAZU7cbgBdV9UtV3VM5AL8BrgVmA5/ihMAB4BdAhKp+idNp4afe8euAEd51/g9wCtiLswvutUZqWAa8B3wO7MJptfnuApwPvAH8HTgCvADE+Ez/IzAM271n2imxCxYaE5pEZDzOrr6+av/oph2yFpQxIUhEooCfAM9bOJn2ygLKmBAjIoOAQzidORa4XI4xzWa7+IwxxgQla0EZY4wJSkH3O6iuXbtqamqq22UYY4xpJTk5OftV9YzG5ms0oERkIfAtYJ+qDq1jugBP4nStLQZmqOoa77QbgAe9s/5cVf/Y2POlpqaSnZ3d2GzGGGPaKRHZ5c98/uziewmY1MD0S4EB3mEm8Iy3gCTgYZwTX44EHhaRRH+KMsYYYxoNKFVdgfODw/pMBl72nqzyIyBBRHoC3wD+4T3x5UHgHzQcdMYYP6kqFVpBeUU5peWlnCw7yYnSExSXFnOq/BTlFeVYByjT3gXiGFRvav66Pc87rr7xpxGRmTitL84888wAlGTM6coryikuLa4ajpcer75/6niD42uMKz3OybKTVGhFjUHR08ZVTdP6pzVnWcW/8ImMiCQyIhKPeKruR0ZE4omo9bgl08W/+aXqFILBQ9EG/zYN/V1O+9vQvL99Q89T+SWjss6m3AKtssylZ1/KwskL2+TvExSdJFT1WeBZgKysLPva14ZKykr44sAXHD111O1S6lRaXloVDg2GSFn9QVM57mT5ySY/f0dPRzpFdSK2Q6xzG+XcdozsiEc8REgEERKBiFTdrz0IzZvWnPUClGs5ZRVllFc4t5VD5fh6H9czf3FpsX/rq2d6hVYE+m3Ravz9uzT2t2npeirfWyJSFeyV95tyC7R8mVrT03ukt9nfIxABtRvntP+VUrzjdgMX1Rr/rwA8n2miCq1g95HdbC3aytb9W/m86HO2Fjm3uYdy/f42HmwiIyKrAsM3RDpFdSIpJql6vHce3+n1Lec7vlNUJzwRHrdfZrtXuSsyWPkGhQkugQiot4HbRGQxToeIw6paICLLgMd8OkZcAtwfgOcz9ThUcsgJn1oh9HnR55woqz6ZdWxULOd2PZdRKaO4fsT1nJN8DkkxSS5WXj/fEKrdionyRLldnvFDhEQQ4bGfXJqm86eb+SKcllBXEcnD6ZkXBaCqvwfewelivh2nm/mN3mkHRORRnLM9Azyiqg11tjB+OFV+ih0Hd5wWQluLtrLv+L6q+TzioV9iP85JPoev9fsa5ySfw7nJ53Ju13PpGdfTvi0aY4Je0J3qKCsrS8P9d1CqSsGxgjpDaOfBnZRr9e6SbrHdqsLHN4TOSjyLDp4OLr4KY4ypm4jkqGpWY/MFRSeJcHX05NGqXXBbi7bW2CV37NSxqvliImMYkDyA9B7pfG/I96pCaEDSABJj7KdlxpjQZAHVhv685c/8/Yu/V3VWKDhWUDVNEPom9OXc5HMZ02dMVQidk3wOKZ1TqnpoGWNMuLCAaiNrC9bynTe+Q2J0IgO7DuSS/pfUCKGzk84mOjK6VWtQhePH4fDh0wdV6NgRoqP9u42KAjuM5T5VOHAA8vOdobAQSkuhrMwZysur7wf6cVPm9XjgggvgG99whj59Gn9txlhAtZHHVj1G546d2fGTHSREJzR5+YbC5fBhOHSo/mmVw5EjzodGIIicHlz+hltTbuPiIDkZkpKgSxeICKOG5LFj1cGze3fd9/Pz4WTTf96FCERGVg8eT83HdY2ra57oaP/mP34cli+HJUuc5x882AmqSZNg3DiIiWm4XtP2KiqguNj52x0/7rwfjx+Hzp1hyJC2qcECqg1sLtzMW5vf4o7hj7L7iwQ2NxIkzQ2XiAjnQ9x3OPNM5zYh4fRpvkNEBJSUOB92/tz6M8+xY7B/f/3LlpU1bTtGRDhBlZxcHVqV9+sbl5QEnTo1/2/XGk6dgoKCusPG9/GRI6cvGxsLvXtDr15Oi6RXL2eoHHfGGU64NxQWHo87Qa8KmzfDsmXw3nvwu9/B//yPE3IXXVQdWOeea61zf6k6/0+1Q6Suxw1Nq+txcXHdz3nFFbB0adu8PuvF10pKS2H9evjoI/j1mx+ya2NP9MBZ9c7v8TQcIP4MsbHt6x+7vLzxkDt6FIqKnOHAger7tR/X988EzgdgQyFW17jEROfDvCkqKmDfvsZbPYWFpy8bFXV62NS+36uX8+01VBQXwwcfVAfW1q3O+DPPrA6riROd93Y4qKiAHTtg7Vrns2P//sZD5PhxZzl/eTzO50RcnHNbOfg+bmhabCykpMDQ065r0TT+9uKzgAqQ/HwnjD76CP7zH8jOdj5gAYjPZ8Dw/dwyeTh9+9YdLp06ta9wCTYlJY2HWO1xBw403Irr0qX+IIuMPL0VtGfP6esTge7dGw+f5OTw2n1Zl127qsPqn/90WpAeD5x/fvWxq8zM0NhOp045rcm1a2Hduurbo94zjnk8znuiKeHRWLDExUGHDsHxOWMB1YpOnnTeUJVh9NFH8OWXzrQOHSAjw/mnGj0a/nz0HpbmP03uHTvpGd/T3cJNDarOh2BDIVbX48OHneUTE+tv6VQ+7t7daR2Zpiktdf6vli1zhsqPhK5d4etfd1pXl1wCPXq4W6c/jh1zWkRr11YPmzY5IQXOl9MRIyA9vXoYMsRp+YcqC6gAUYWvvqoOoo8+gjVrqt9cffs6QTR6tBNKaWnOMQCALw9/Sf+n+nNr5q385rLfuPciTECVlTkfoHZgv+0UFsI//uG0rv7+d9i71xmfllbduhozxvmC6KZ9+2oG0dq1sH278zkCTsD6BlF6Opx9ttNiCicWUM1UXAw5OTV31xV4f64UEwNZWdVhNGqU8y25Pj/62494bs1zbL99O2d2scuIGBMIFRVOi6SydbVqlfOlIS4OJkxwWlff+Ab07996NajCzp3VIVS5my4/v3qe1FQngNLSqsOod+/g2MXmNgsoP6g6ByV9d9WtX199HKF//+owGj0ahg/3f3dN/tF8znryLK4bfh3PXf5c670IY8Lc0aNOF/b33nOGnTud8f37V4fVhAlOgDVHaSl89lnNVtG6ddW7ej0eGDiwZqsoLc3ZBWzqZgFVh2PHYPXqmrvrKntUxcXByJHVu+tGj3a67DbXT5f9lAUfL+Dz2z6nf1IrfpUzxlRRdXapVbau3n/f2SsSFQVjx1b3Dhw+vO6WzPHjsGFDzTDauLH6t2YxMc6yvkE0bJjt7m2qsA+oigrYtq1mGH36aXWXzIEDax47GjIkcPuBC48XkvpkKlcNuopXrnwlMCs1xjTZyZPw4YdOy2rZMid8wOlcUdmy8j1utHVr9fGixMTTjxedc07Tf35gThe2AbVzJ/zwh/Dxx3DwoDOuSxfneFFlGI0c6XQXbi0P/PMBHl/1OJt+uIlBZwxqvScyxjRJfr7TyWLZMuf2gPcCQH36nB5GffrY8aLWErZnM09Kgrw8+M53qo8dDRzYdr+dOHjiIE9/8jRTBk+xcDImyPTqBTNmOEN5OWzZAj17Or85MsEn5AKqSxdnV55bnv7kaY6eOsoD4x5wrwhjTKM8npafEcG0rhD4TXbwOHryKAs+WsC3z/k2I3qMcLscY4xp1yygAuiZ7Gc4WHKQB8c/6HYpxhjT7llABUhxaTG//s+vuaT/JYzsPdLtcowxpt2zgAqQ53KeY9/xfTw4zlpPxhgTCBZQAXCy7CS//PcvGd93POP6jnO7HGOMCQkh14vPDS+te4n8o/m8NPklt0sxxpiQYS2oFiotL2Xeh/MY1XsUF591sdvlGGNMyLAWVAu99ulr5B7K5elLn0bsZ+fGGBMw1oJqgfKKch5b+Rgjuo/gmwO+6XY5xhgTUqwF1QJvbn6TbQe28eZ337TWkzHGBJi1oJqpQiuYu3Iug7oO4qpBV7ldjjHGhBxrQTXT21vfZuO+jbx65atEiOW8McYEmn2yNoOq8uiKR+mf2J/vDf2e2+UYY0xIshZUM7y3/T3WFKzh+W8/T2SEbUJjjGkN1oJqosrWU5/OfbhuxHVul2OMMSHLr4ASkUkislVEtovI7Dqm9xWRf4rIBhH5l4ik+EwrF5F13uHtQBbvhn/l/ov/5P2H+8bcRwdPB7fLMcaYkNXo/ikR8QC/Bb4O5AGrReRtVd3sM9sTwMuq+kcR+RrwOFDZvDihqmkBrts1j654lB5xPfh+xvfdLsUYY0KaPy2okcB2Vd2hqqeAxcDkWvMMBt733l9ex/SQ8OGXH7I8dzn3XHAP0ZHRbpdjjDEhzZ+A6g185fM4zzvO13qg8sdAVwLxIpLsfRwtItki8pGIXFHXE4jITO882YWFhU0ov23NXTmX5Jhkbs281e1SjDEm5AWqk8TdwIUisha4ENgNlHun9VXVLOAaYIGI9K+9sKo+q6pZqpp1xhlnBKikwMrJz+Hd7e9y1/l3Edsh1u1yjDEm5PnTR3o30MfncYp3XBVVzcfbghKROOA7qnrIO22393aHiPwLSAe+aHHlbWzuyrkkRCdw28jb3C7FGGPCgj8tqNXAABHpJyIdgKlAjd54ItJVpOp0CvcDC73jE0WkY+U8wBjAt3NFu/Dp3k9Z+tlSbh95O507dna7HGOMCQuNBpSqlgG3AcuALcAbqrpJRB4Rkcu9s10EbBWRz4HuwFzv+EFAtoisx+k8Ma9W77924bFVjxHXIY7bR93udinGGBM2/DoNgqq+A7xTa9xDPveXAEvqWO7fwLAW1uiqrfu38qeNf+KeC+4huVNy4wsYY4wJCDuTRCPmfTiP6Mho7jr/LrdLMcaYsGIB1YDcQ7m8sv4VZmbOpHtcd7fLMcaYsGIB1YB5q+bhifBw9wV3u12KMcaEHQuoeuQdyePFdS9yY9qNpHROaXwBY4wxAWUBVY8n/v0E5RXl3DfmPrdLMcaYsGQBVYe9x/bybM6zXDfiOvol9nO7HGOMCUsWUHWY/5/5lJSVcP/Y+90uxRhjwpZdDraWouIifpf9O7439Huck3yO2+UYY9pIaWkpeXl5lJSUuF1KyIiOjiYlJYWoqKhmLW8BVctTHz/FsVPHmDN2jtulGGPaUF5eHvHx8aSmpiIibpfT7qkqRUVF5OXl0a9f8w6V2C4+H4dLDvPUJ09xxcArGNa9XZ8AwxjTRCUlJSQnJ1s4BYiIkJyc3KIWqQWUj9+t/h2HSg7x4LgH3S7FGOMCC6fAaun2tIDyOn7qOPM/ms+lZ19KZq9Mt8sxxpiwZwHl9YecP7C/eD8PjrfWkzGm7RUVFZGWlkZaWho9evSgd+/eVY9PnTrl1zpuvPFGtm7d2sqVth3rJAGUlJXwq3//igmpE7igzwVul2OMCUPJycmsW7cOgJ/97GfExcVx9901T7OmqqgqERF1ty1efPHFVq+zLVkLCli4diF7ju2x1pMxJuhs376dwYMHc+211zJkyBAKCgqYOXMmWVlZDBkyhEceeaRq3rFjx7Ju3TrKyspISEhg9uzZjBgxgvPPP599+/a5+CqaJ+xbUKfKT/GLD3/BBX0uYELqBLfLMcYEgTveu4N1e9YFdJ1pPdJYMGlBs5b97LPPePnll8nKygJg3rx5JCUlUVZWxoQJE5gyZQqDBw+usczhw4e58MILmTdvHnfddRcLFy5k9uzZLX4dbSnsW1CvrH+FLw9/yYPjHrQePMaYoNS/f/+qcAJYtGgRGRkZZGRksGXLFjZvPv1C5TExMVx66aUAZGZmkpub21blBkxYt6DKKsp4fNXjZPTMYNLZk9wuxxgTJJrb0mktsbGxVfe3bdvGk08+ySeffEJCQgLTp0+v87dGHTp0qLrv8XgoKytrk1oDKaxbUH/a+Ce+OPiFtZ6MMe3GkSNHiI+Pp3PnzhQUFLBs2TK3S2o1YduCqtAK5q6cy9BuQ5k8cLLb5RhjjF8yMjIYPHgwAwcOpG/fvowZM8btklqNqKrbNdSQlZWl2dnZrf48SzYv4btvfpdF31nE1KFTW/35jDHBbcuWLQwaNMjtMkJOXdtVRHJUNaueRaqE5S4+VeXnK37OgKQBfHfwd90uxxhjTB3Cchff37b9jfV71/Pi5BfxRHjcLscYY0wdwq4FVdl6Sk1I5dph17pdjjHGmHqEXQvqnzv/yce7P+b33/w9UZ7mXUTLGGNM6wu7FtSjKx6lV3wvZqTNcLsUY4wxDQirgFqxawUrdq3g3gvupWNkR7fLMcYY04CwCqi5K+fSLbYbt2Te4nYpxhhTw4QJE0770e2CBQuYNWtWvcvExcUBkJ+fz5QpU+qc56KLLqKxn+4sWLCA4uLiqseXXXYZhw4d8rf0VhM2AfXJ7k/4+xd/56fn/5ROUZ3cLscYY2qYNm0aixcvrjFu8eLFTJs2rdFle/XqxZIlS5r93LUD6p133iEhIaHZ6wuUsAmon6/4OYnRiczKqv/biDHGuGXKlCn87W9/q7o4YW5uLvn5+aSnpzNx4kQyMjIYNmwYf/3rX09bNjc3l6FDhwJw4sQJpk6dyqBBg7jyyis5ceJE1XyzZs2qukzHww8/DMBTTz1Ffn4+EyZMYMIE54oOqamp7N+/H4D58+czdOhQhg4dyoIFC6qeb9CgQdxyyy0MGTKESy65pMbzBEpY9OJbt2cd//v5//LfF/038R3j3S7HGBPk7rgD1gX2ahukpcGCBs5Bm5SUxMiRI3n33XeZPHkyixcv5uqrryYmJoalS5fSuXNn9u/fz+jRo7n88svrPX/oM888Q6dOndiyZQsbNmwgIyOjatrcuXNJSkqivLyciRMnsmHDBm6//Xbmz5/P8uXL6dq1a4115eTk8OKLL/Lxxx+jqowaNYoLL7yQxMREtm3bxqJFi3juuee4+uqreeutt5g+fXpAtlUlv1pQIjJJRLaKyHYROe2CIiLSV0T+KSIbRORfIpLiM+0GEdnmHW4IZPH+emzlY8R3iOfHI3/sxtMbY4xffHfzVe7eU1XmzJnD8OHDufjii9m9ezd79+6tdx0rVqyoCorhw4czfPjwqmlvvPEGGRkZpKens2nTpjov0+Fr1apVXHnllcTGxhIXF8dVV13FypUrAejXrx9paWlA613Oo9EWlIh4gN8CXwfygNUi8raq+r6yJ4CXVfWPIvI14HHgOhFJAh4GsgAFcrzLHgz0C6nPlsItLNm8hNljZ5MYk9hWT2uMaccaaum0psmTJ3PnnXeyZs0aiouLyczM5KWXXqKwsJCcnByioqJITU2t8/Iajdm5cydPPPEEq1evJjExkRkzZjRrPZU6dqzuCe3xeFplF58/LaiRwHZV3aGqp4DFQO3Tfw8G3vfeX+4z/RvAP1T1gDeU/gG06YWXHl/1ODFRMdw5+s62fFpjjGmyuLg4JkyYwE033VTVOeLw4cN069aNqKgoli9fzq5duxpcx/jx43n99dcB2LhxIxs2bACcy3TExsbSpUsX9u7dy7vvvlu1THx8PEePHj1tXePGjeMvf/kLxcXFHD9+nKVLlzJu3LhAvdxG+RNQvYGvfB7necf5Wg9c5b1/JRAvIsl+LouIzBSRbBHJLiws9Lf2Rn1x4Ate//R1fpD5A86IPSNg6zXGmNYybdo01q9fXxVQ1157LdnZ2QwbNoyXX36ZgQMHNrj8rFmzOHbsGIMGDeKhhx4iMzMTgBEjRpCens7AgQO55ppralymY+bMmUyaNKmqk0SljIwMZsyYwciRIxk1ahQ333wz6enpAX7F9Wv0chsiMgWYpKo3ex9fB4xS1dt85ukF/AboB6wAvgMMBW4GolX15975/gs4oapP1Pd8gbzcxi1v38IrG15hx0920Cu+V0DWaYwJTXa5jdbR2pfb2A308Xmc4h1XRVXzVfUqVU0HHvCOO+TPsq3ly8Nf8sf1f+T76d+3cDLGmHbIn4BaDQwQkX4i0gGYCrztO4OIdBWRynXdDyz03l8GXCIiiSKSCFziHdfqfvXhr1CU+8be1xZPZ4wxJsAaDShVLQNuwwmWLcAbqrpJRB4Rkcu9s10EbBWRz4HuwFzvsgeAR3FCbjXwiHdcq9pzbA/PrXmOG0bcwJldzmztpzPGhIhgu8J4e9fS7enXD3VV9R3gnVrjHvK5vwSo8zwbqrqQ6hZVm3ji309QWlHK7LGn/WTLGGPqFB0dTVFREcnJyfX+CNb4T1UpKioiOjq62esIuTNJ7C/ezzPZzzBt6DTOTjrb7XKMMe1ESkoKeXl5BLIncbiLjo4mJSWl8RnrEXIBVVxazGUDLmPOuDlul2KMaUeioqLo16+f22UYHyEXUGd2OZM3v/um22UYY4xpobA5m7kxxpj2xQLKGGNMUGr0TBJtTUQKgYZPNuWfrsD+AKwnXNj2ahrbXk1j26vpQnmb9VXVRs8/F3QBFSgiku3PqTSMw7ZX09j2ahrbXk1n28x28RljjAlSFlDGGGOCUigH1LNuF9DO2PZqGtteTWPbq+nCfpuF7DEoY4wx7Vsot6CMMca0YxZQxhhjglLIBZSITBKRrSKyXUTsdOYNEJE+IrJcRDaLyCYR+YnbNbUXIuIRkbUi8v/criXYiUiCiCwRkc9EZIuInO92TcFMRO70/j9uFJFFItL804G3cyEVUCLiAX4LXAoMBqaJyGB3qwpqZcBPVXUwMBr4kW0vv/0E5/popnFPAu+p6kBgBLbd6iUivYHbgSxVHQp4cC4SG5ZCKqCAkcB2Vd2hqqeAxcBkl2sKWqpaoKprvPeP4nxw9Ha3quAnIinAN4Hn3a4l2IlIF2A88AKAqp5S1UPuVhX0IoEYEYkEOgH5LtfjmlALqN7AVz6P87APXL+ISCqQDnzsbiXtwgLgXqDC7ULagX5AIfCid5fo8yIS63ZRwUpVdwNPAF8CBcBhVf27u1W5J9QCyjSDiMQBbwF3qOoRt+sJZiLyLWCfqua4XUs7EQlkAM+oajpwHLBjw/UQkUScvT79gF5ArIhMd7cq94RaQO0G+vg8TvGOM/UQkSiccHpNVf/sdj3twBjgchHJxdmF/DURedXdkoJaHpCnqpUt8yU4gWXqdjGwU1ULVbUU+DNwgcs1uSbUAmo1MEBE+olIB5yDi2+7XFPQEhHBOTawRVXnu11Pe6Cq96tqiqqm4ry/3lfVsP2G2xhV3QN8JSLnekdNBDa7WFKw+xIYLSKdvP+fEwnjTiUhdUVdVS0TkduAZTi9Xxaq6iaXywpmY4DrgE9FZJ133BxVfcfFmkzo+THwmvdL4w7gRpfrCVqq+rGILAHW4PSyXUsYn/LITnVkjDEmKIXaLj5jjDEhwgLKGGNMULKAMsYYE5QsoIwxxgQlCyhjjDFByQLKGGNMULKAMsYYE5QsoIwxxgQlCyhjjDFByQLKGGNMULKAMsYYE5QsoIwxxgQlCyhjjDFByQLKmAATkVwRudjtOoxp7yygjDHGBCULKGPaiIjcIiLbReSAiLwtIr2840VE/kdE9onIERH5VESGeqddJiKbReSoiOwWkbvdfRXGtB0LKGPagIh8DXgcuBroCewCFnsnXwKMB84BunjnKfJOewG4VVXjgaHA+21YtjGuCqlLvhsTxK4FFqrqGgARuR84KCKpQCkQDwwEPlHVLT7LlQKDRWS9qh4EDrZp1ca4yFpQxrSNXjitJgBU9RhOK6m3qr4P/Ab4LbBPRJ4Vkc7eWb8DXAbsEpEPROT8Nq7bGNdYQBnTNvKBvpUPRCQWSAZ2A6jqU6qaCQzG2dV3j3f8alWdDHQD/gK80cZ1G+MaCyhjWkeUiERXDsAi4EYRSRORjsBjwMeqmisi54nIKBGJAo4DJUCFiHQQkWtFpIuqlgJHgArXXpExbcwCypjW8Q5wwme4CPgv4C2gAOgPTPXO2xl4Duf40i6cXX+/8k67DsgVkSPAD3COZRkTFkRV3a7BGGOMOY21oIwxxgQlCyhjjDFByQLKGGNMULKAMsYYE5SC7kxO8QDlAAAZpklEQVQSXbt21dTUVLfLMMYY00pycnL2q+oZjc0XdAGVmppKdna222UYY4xpJSKyq/G5bBefMcaYIBVyAXX05FEeX/k4e4/tdbsUY4wxLRByAbXn2B4eXP4gv/zwl26XYowxpgWC7hhUSw1IHsD04dN5JvsZ7hlzDz3ierhdkjGmHSgtLSUvL4+SkhK3SwkZ0dHRpKSkEBUV1azlQy6gAB4c9yCvbXiNX374S+Z/Y77b5Rhj2oG8vDzi4+NJTU1FRNwup91TVYqKisjLy6Nfv37NWkfI7eKDmq2oPcf2uF2OMaYdKCkpITk52cIpQESE5OTkFrVIQzKgAB4c/yCl5aX8YtUv3C7FGNNOWDgFVku3Z8gG1NlJZ3PdiOv4fc7vKTha4HY5xhhjmihkAwqcY1Gl5aX84kNrRRljgltRURFpaWmkpaXRo0cPevfuXfX41KlTfq3jxhtvZOvWra1cadsJyU4Slfon9ef6Edfzh5w/cN+Y++gZ39Ptkowxpk7JycmsW7cOgJ/97GfExcVx991315hHVVFVIiLqblu8+OKLrV5nWwrpFhTAA+MeoLS8lHmr5rldijHGNNn27dsZPHgw1157LUOGDKGgoICZM2eSlZXFkCFDeOSRR6rmHTt2LOvWraOsrIyEhARmz57NiBEjOP/889m3b5+Lr6J5QroFBU4r6oYRNzitqLH30Su+l9slGWOC3B3v3cG6PesCus60HmksmLSgWct+9tlnvPzyy2RlZQEwb948kpKSKCsrY8KECUyZMoXBgwfXWObw4cNceOGFzJs3j7vuuouFCxcye/bsFr+OthTyLSiAB8Y/QLmWWyvKGNMu9e/fvyqcABYtWkRGRgYZGRls2bKFzZs3n7ZMTEwMl156KQCZmZnk5ua2VbkBE/ItKICzEs/ihhE38GzOs9w35j56d+7tdknGmCDW3JZOa4mNja26v23bNp588kk++eQTEhISmD59ep2/NerQoUPVfY/HQ1lZWZvUGkhh0YIC51hUuZZbjz5jTLt25MgR4uPj6dy5MwUFBSxbtsztklpN2ARUv8R+zBgxg2dznmX3kd1ul2OMMc2SkZHB4MGDGThwINdffz1jxoxxu6RWI6ra/IVFJgFPAh7geVWdV2v6XcDNQBlQCNykqg1eqCorK0tb64KFuYdyGfD0AH6Q+QOevuzpVnkOY0z7tGXLFgYNGuR2GSGnru0qIjmqmlXPIlWa3YISEQ/wW+BSYDAwTUQG15ptLZClqsOBJYCr18BITUjlxrQbeXbNs+QdyXOzFGOMMY1oyS6+kcB2Vd2hqqeAxcBk3xlUdbmqFnsffgSktOD5AmLOuDlUaIX16DPGmCDXkoDqDXzl8zjPO64+3wfebcHzBURqQio3pd3Ec2ue46vDXzW+gDHGGFe0SScJEZkOZAG/qmf6TBHJFpHswsLCVq9nzrg5qKq1oowxJoi1JKB2A318Hqd4x9UgIhcDDwCXq+rJulakqs+qapaqZp1xxhktKMk/fRP6clP6TTy/9nlrRRljTJBqSUCtBgaISD8R6QBMBd72nUFE0oE/4IRTUJ0IqrIV9fiqx90uxRhjTB2aHVCqWgbcBiwDtgBvqOomEXlERC73zvYrIA54U0TWicjb9ayuzZ3Z5Uy+n/59nl/zPF8e/tLtcowxYW7ChAmn/eh2wYIFzJo1q95l4uLiAMjPz2fKlCl1znPRRRfR2E93FixYQHFxcdXjyy67jEOHDvlbeqtp0TEoVX1HVc9R1f6qOtc77iFVfdt7/2JV7a6qad7h8obX2LbuH3c/AI+vtFaUMcZd06ZNY/HixTXGLV68mGnTpjW6bK9evViyZEmzn7t2QL3zzjskJCQ0e32BEjZnkqhLZSvqhbUvsOtQg78fNsaYVjVlyhT+9re/VV2cMDc3l/z8fNLT05k4cSIZGRkMGzaMv/71r6ctm5uby9ChQwE4ceIEU6dOZdCgQVx55ZWcOHGiar5Zs2ZVXabj4YcfBuCpp54iPz+fCRMmMGHCBABSU1PZv38/APPnz2fo0KEMHTqUBQsWVD3foEGDuOWWWxgyZAiXXHJJjecJlLA4WWxD5oybwwtrX+DxVY/z+2/93u1yjDFB4I47YF1gr7ZBWhosaOActElJSYwcOZJ3332XyZMns3jxYq6++mpiYmJYunQpnTt3Zv/+/YwePZrLL78cEalzPc888wydOnViy5YtbNiwgYyMjKppc+fOJSkpifLyciZOnMiGDRu4/fbbmT9/PsuXL6dr16411pWTk8OLL77Ixx9/jKoyatQoLrzwQhITE9m2bRuLFi3iueee4+qrr+att95i+vTpAdlWlcK6BQXQp0sfbs64mYVrF1oryhjjKt/dfJW791SVOXPmMHz4cC6++GJ2797N3r17613HihUrqoJi+PDhDB8+vGraG2+8QUZGBunp6WzatKnOy3T4WrVqFVdeeSWxsbHExcVx1VVXsXLlSgD69etHWloa0HqX8wj7FhTA/WPv54W1L/DYysf4w7f/4HY5xhiXNdTSaU2TJ0/mzjvvZM2aNRQXF5OZmclLL71EYWEhOTk5REVFkZqaWuflNRqzc+dOnnjiCVavXk1iYiIzZsxo1noqdezYseq+x+NplV18Yd+CAm8rKv1mFq5bSO6hXLfLMcaEqbi4OCZMmMBNN91U1Tni8OHDdOvWjaioKJYvX86uXQ3v6Rk/fjyvv/46ABs3bmTDhg2Ac5mO2NhYunTpwt69e3n33eoT+8THx3P06NHT1jVu3Dj+8pe/UFxczPHjx1m6dCnjxo0L1MttlAWU1/3j7idCInhs5WNul2KMCWPTpk1j/fr1VQF17bXXkp2dzbBhw3j55ZcZOHBgg8vPmjWLY8eOMWjQIB566CEyMzMBGDFiBOnp6QwcOJBrrrmmxmU6Zs6cyaRJk6o6SVTKyMhgxowZjBw5klGjRnHzzTeTnp4e4FdcvxZdbqM1tOblNhpz2zu38YecP7Dtx9tITUh1pQZjjDvschutw5XLbYSi+8c6rai5K+a6XYoxxoQ9CygfvTv3ZmbGTF5a/xI7D+50uxxjjAlrFlC1zB47G494mLvSWlHGhJtgO+TR3rV0e1pA1dK7c29mZs7kj+v/yI6DO9wuxxjTRqKjoykqKrKQChBVpaioiOjo6GavwzpJ1CH/aD5nPXkW1w67lhcmv+BqLcaYtlFaWkpeXl6LfhtkaoqOjiYlJYWoqKga4/3tJGE/1K1Dr/he3Jp5K79d/VseGP8AZyWe5XZJxphWFhUVRb9+/dwuw/iwXXz1mD12NlGeKH6+4udul2KMMWHJAqoePeN7cmvmrby8/mW+OPCF2+UYY0zYsYBqwH1j7nNaUSutFWWMMW3NAqoBPeN78oPMH/DK+lfYfmC72+UYY0xYsYBqxH1j77NjUcYY4wILqEb0iOvBrKxZvLrhVWtFGWNMG7KA8sO9Y+6lg6cDj6541O1SjDEmbFhA+cG3FbWtaJvb5RhjTFiwgPLTvWPupaOno7WijDGmjVhA+al7XHd+eN4Pee3T1/i86HO3yzHGmJBnAdUE91xwDx09Ha1HnzHGtAELqCboHtedH533I1779DW27t/qdjnGGBPSLKCa6J4x9xAdGW1nlzDGmFbWooASkUkislVEtovI7DqmjxeRNSJSJiJTWvJcwaJbbDd+dN6PeP3T160VZYwxrajZASUiHuC3wKXAYGCaiAyuNduXwAzg9eY+TzC6+4K7iY6Mth59xhjTilrSghoJbFfVHap6ClgMTPadQVVzVXUDUNGC5wk63WK7cdt5t7Fo4yI+2/+Z2+UYY0xIaklA9Qa+8nmc5x3XZCIyU0SyRSS7sLCwBSW1nbsvuJuYyBhrRRljTCsJik4SqvqsqmapatYZZ5zhdjl+OSP2DG4beRuLPl3ElsItbpdjjDEhpyUBtRvo4/M4xTsubNx9wd10iupkrShjjGkFLQmo1cAAEeknIh2AqcDbgSmrfejaqSs/HvljFm9czObCzW6XY4xpgk2bYN48WLQIdofVV+v2o9kBpaplwG3AMmAL8IaqbhKRR0TkcgAROU9E8oDvAn8QkU2BKDqY/PSCnxLbIdZaUca0A4WF8NRTkJUFQ4fC/ffDNddASgqcfTbcdBO89BLs3AmqbldrRIPsr5CVlaXZ2dlul9Ekc/45h3mr5vHprE8Z0m2I2+UYY3ycPAn/+7/w8svw7rtQVgbp6XDDDXD11VBQAB98ACtWOMOBA85yKSlw4YUwfrxze845IOLuawkVIpKjqlmNzmcB1XJFxUWkPpnKNwd8k8VTFrtdjjFhTxU++sgJpT/9CQ4ehJ49Yfp0uO46GDas7uUqKmDz5urA+uAD2LvXmdatW3VYjR/vtMAigqKbWftjAdXGHvjnAzy+6nFrRRnjotxcePVVJ5i2bYOYGLjySqe1NHEieDxNW5+qs57KsPrgA/jK++OaxEQYN646tNLSIDIy4C8pJFlAtbGi4iL6PdmPSwdcyp+m/MntcowJG0eOwJIlTih98IEz7qKL4Prr4Tvfgc6dA/t8ubnVuwM/+AC2b3fGx8fDmDHVgZWVBR06BPa5Q0XYBpQqfPopDBnS9G9LLfXg+w/y2MrH2DBrA0O7DW3bJzcmjJSXw//9nxNKS5fCiRMwYIATStOnQ2pq29WSn18zsDZ7O/TGxMDo0dW7BEePdsaZMA6onTvhrLOgSxen+V355sjIaP3md2UratLZk3jju2+07pMZE4Y2bnRC6dVXnc4NiYkwdaoTTKNGBUcnhsJCWLWq+jjWunXOF+eoKBg5svoz6YILnFZXOArbgDpyxOmxU7m/+HPvxW/j4mo2v887r3Wa3w++/yBzV85lww82MKx7PUdijTF+27cPXn/dCaa1a50vmpdd5oTSt74FHTu6XWHDDh2CDz+sbmFlZzstQI/H+eJc+Zk0dqwTuOEgbAOqtoICWLmyOrA2eX+JFR0N55/vvDEuvND59hWI5veBEwdIXZDKN87+Bm9+982Wr9CYMFRSUrNreHk5ZGY6nR2mToV2cka0Oh07Bv/5T3VgffwxnDrltP6GD3cCa/x46N8fEhKcvUFdurT9IYvWZAFVj/37qwPLt/ndocPpze+4uOY9x3+9/1/8fOXPrRVlTBOoOh/clV3DDx2C3r2ru4YPCdHOsSUl8Mkn1Z9J//43FBefPl98vBNYlaFVeb++cb6Pu3QJrg4bFlB+OnTI2V9c+W0mJ8f5thYZ6Xxj821+d+ni3zoPnDhAvyf78fWzvs6Sq5e07gswpp3buRNeecUJpi++gE6d4KqrnF14X/taaLUc/HHqlPPFefduOHzY+YyqHOp7fPiw8xuuhnTq1PRg8x0XHR24Y3wWUM109KjzLa5yl+Ann0BpqfODvBEjqncJjhsHycn1r+eh5Q/x6IpHWf+D9QzvPrztXoAJWapw/LhzEL6+4ehRp+VfuVuoSxenm3Vd97t0gdhYdzoWHD5c3TV8xQqnhgkTnFC66qrw7TzQXBUVzq7D2gHWUKj5Pj540DnDRkM6dHDC6vLL4bnnWlavBVSAFBc7+4grm9//+Y/TJAfnl+S+p0Lp3r16uYMnDpL6ZCoXn3Uxb139ljvFm6Cm6nTqqR0y+/bVH0CV773aOnZ0jsvExzsfVEeOOENj/94REfUHWO0wa2g+f3rIlpXV7BpeUgLnnlvdNfzMM5u+DU1gqDpd9f0JtCFD4Mc/btnzWUC1kpMnYfXq6sD68EPnWy04/2yVYXXhhfDctod5ZMUjrLt1HSN6jHC3cNPqKiqcf+DGQqZy2L/f2Z1Tl06dnMCpPXTrVvf4uLjTW0K+36qPHHFua9/353FpaeOvvXL3UX1BduoUvPUW7NkDSUkwbZoTTOedFxxdw03bsoBqI6WlsGZN9TGslSudf3CAvqnl5CcvZth5B1hy749JTW3f/4wVFc7xOX+Gpszb0LIVFc6gWv9tQ9P8mac5y5eU1B045eV1b7v4+IYDpvbQqVPb/m0bUlLiX5g1NK20tLpr+GWXBX/XcNO6LKBcUl4O69dXB9ay94s5ccT5tElJcT6gKj/woOH7jU0P5P3agVJXwISbiAjnC4VI9f3K28pdav4O4f6BrNq+v5yZwPI3oOzUhgFW+eO7jAy44w44UHyKvnMmctaRGxlSMpOjR535Kj/4mnK/ucv5cz8iwqm9cqj9uKlDoJaPiKgefAOirtAI1Dy+28YEhm1P0xwWUK0sqVMCd18xiZ99cCsvzTyP9J7pbpdkjDHtgl3NpA38ZPRPSIhO4L8/+G+3SzHGmHbDAqoNJEQncOfoO/nr1r/yyvpX2HNsj9slGWNM0LNOEm3kcMlhhj4zlLwjeQD0ju9NVq8sMntmOre9MukW283lKo0xpvVZJ4kg0yW6C5/96DPWFKwhpyCH7PxssvOz+evWv1bNc2aXM6sCqzK8kjs1cLoKY4wJYdaCctmRk0dYW7DWCawCJ7S2H9heNT01IdUJrJ5OaGX0zCAxJkzOyW+MCUn2O6h27FDJIdYUrKlqZeUU5LDj4I6q6f0T+9doZWX0zKBLtJ9nsjXGGJdZQIWYAycOkJOfUxVY2fnZ7Dq8q2r6Ocnn1Dimld4jnfiOdsZNY0zwsYAKA4XHC8kpyHGCy7t7sLIThiAM7DqQzF6ZVbsH03qkEdsh1uWqjTHhzgIqTO09trdGJ4zs/GwKjhUAECERDOo6qGr3YFavLEZ0H0FMVAAuJWyMMX6ygDJV8o/m19g9uDp/NfuO7wPAIx6GdBtC7/jeeCI8eMRT721kRGSD02vM18g8/s7b0dORTlGdqoaYqBjnNjIGT0SYXcnOmBDRJt3MRWQS8CTgAZ5X1Xm1pncEXgYygSLge6qa25LnNE3XK74Xvc7txbfP/TYAqsruo7udwPLuHiwsLqS8opxyLa/ztqyirN5pvrdtqXZ4+TPERMY0af7oyGjETiQXFCq0grKKsiYPpeWlDU73RHiIiogiMiKSyIhIojzO/cpxlY/rGld7OY947P0SQM0OKBHxAL8Fvg7kAatF5G1V3ewz2/eBg6p6tohMBX4BfK8lBZuWExFSOqeQ0jmFKwZeEbD1qioVWuFXkPkbeifLT3Ki9ATFpcUND2XFNeY7cOLAafOcKDvRrNdVV7hFR0bXaOlFSESDLcUIiageV898ja3D3+cp13Ln7+DdjnXd9/071XXf7+WbsM7mhItvwCjBtbenPrXDrLmh5+97qMb4BsY19P5pyvzdY7szrPuwttmWLVh2JLBdVXcAiMhiYDLgG1CTgZ957y8BfiMiosG2X9EEhIg4b2I8Tps6yFRoBSVlJXUGnL8hWHm/pKyk6kP7ZNnJRoPZ90Pen3kqtMKVbRQhEVUfSr4fWpX3fT+w6rpf1zKV96M8UXSK6lT1wVznIPVP8/1Qb+rgGxa+gyfCU/VlqayijNKK0hqhWPm4rnG+LbMmLVfPtFPlpzh+6njVY9/3R33vr/reVxVa0WqBfsXAK1j6vaWtsu7aWhJQvYGvfB7nAaPqm0dVy0TkMJAM7PedSURmAjMBzrTrPptWEiERVS2gYFdXa7SxkKvQCiq0otlhEiERtnsqhKhqne8Rf8Y19H5Liklqs9cQFKc6UtVngWfB6SThcjnGuC7YW6Mm+IlIVYu0vWrJ2cx3A318Hqd4x9U5j4hEAl1wOksYY4wxDWpJQK0GBohIPxHpAEwF3q41z9vADd77U4D37fiTMcYYf7Tod1AichmwAGcnxEJVnSsijwDZqvq2iEQDrwDpwAFgamWnigbWWQjsamgeP3Wl1rEu0yDbXk1j26tpbHs1XShvs76qekZjMwXdD3UDRUSy/fkhmHHY9moa215NY9ur6Wyb2RV1jTHGBCkLKGOMMUEplAPqWbcLaGdsezWNba+mse3VdGG/zUL2GJQxxpj2LZRbUMYYY9oxCyhjjDFBKeQCSkQmichWEdkuIrPdrieYiUgfEVkuIptFZJOI/MTtmtoLEfGIyFoR+X9u1xLsRCRBRJaIyGciskVEzne7pmAmInd6/x83isgi7+9Jw1JIBZTPJUAuBQYD00RksLtVBbUy4KeqOhgYDfzItpfffgJscbuIduJJ4D1VHQiMwLZbvUSkN3A7kKWqQ3FOgjDV3arcE1IBhc8lQFT1FFB5CRBTB1UtUNU13vtHcT44ertbVfATkRTgm8DzbtcS7ESkCzAeeAFAVU+p6iF3qwp6kUCM9/ylnYB8l+txTagFVF2XALEPXD+ISCrOKak+dreSdmEBcC/gzkWb2pd+QCHwoneX6PMiEut2UcFKVXcDTwBfAgXAYVX9u7tVuSfUAso0g4jEAW8Bd6jqEbfrCWYi8i1gn6rmuF1LOxEJZADPqGo6cBywY8P1EJFEnL0+/YBeQKyITHe3KveEWkD5cwkQ40NEonDC6TVV/bPb9bQDY4DLRSQXZxfy10TkVXdLCmp5QJ6qVrbMl+AElqnbxcBOVS1U1VLgz8AFLtfkmlALKH8uAWK8xLl86gvAFlWd73Y97YGq3q+qKaqaivP+el9Vw/YbbmNUdQ/wlYic6x01EdjsYknB7ktgtIh08v5/TiSMO5W030st1sF7WfnbgGVUXwJkk8tlBbMxwHXApyKyzjtujqq+42JNJvT8GHjN+6VxB3Cjy/UELVX9WESWAGtwetmuJYxPeWSnOjLGGBOUQm0XnzHGmBBhAWWMMSYoWUAZY4wJShZQxhhjgpIFlDHGmKBkAWWMMSYoWUAZY4wJSv8f2eU5Nix5J70AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U99eFGdup6pp"
      },
      "source": [
        "**Evaluation of the model**\n",
        "\n",
        "The model is evaluated against the full test set, printing the score and accuracy. \n",
        "The code also generates a few random sentences from the test set and prints the model's prediction, the label and the actual sentence. This will give us an idea of the quality of the prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C57xXLt0p6pq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b9dd8e-35a2-42c1-f8dc-e718678728ec"
      },
      "source": [
        "# evaluate\n",
        "score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
        "print(\"Test score: %.3f, accuracy: %.3f\" % (score, acc))\n",
        "\n",
        "for i in range(5):\n",
        "    idx = np.random.randint(len(Xtest))\n",
        "    xtest = Xtest[idx].reshape(1,40)\n",
        "    ylabel = ytest[idx]\n",
        "    ypred = model.predict(xtest)[0][0]\n",
        "    sent = \" \".join([index2word[x] for x in xtest[0].tolist() if x != 0])\n",
        "    print(\"%.0f\\t%d\\t%s\" % (ypred, ylabel, sent))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1418/1418 [==============================] - 1s 397us/step\n",
            "Test score: 0.056, accuracy: 0.987\n",
            "0\t0\ti hate harry potter , that daniel wotshisface needs a fucking slap ...\n",
            "1\t1\ti am going to start reading the harry potter series again because that is one awesome story .\n",
            "0\t0\tda vinci code sucks .\n",
            "0\t0\tmy point is , harry potter is evil ...\n",
            "1\t1\ti love harry potter .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rIQKK-orp6px"
      },
      "source": [
        "Conclusions\n",
        "-------------\n",
        "\n",
        "This exercise finishes our short deep leanring course. You know have a pracyicla understanding of how to train a simple MLP (multi-Layer Perceptron), CNNs (Concolutional Neural Networks) with different layers, a simple RNN (Recurrent Neural network) and the more complex LSTM (Long Short Term Neural Network). Well done!\n",
        "\n",
        "**Copyright (c)** 2019 Angelo Cangelosi, MIT License. Code and examples adapted from Gulli & Pal (2017) Deep Learning with Keras. Punkt Publishing"
      ]
    }
  ]
}